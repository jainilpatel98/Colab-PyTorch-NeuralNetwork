{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 19:41:12.955184: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-26 19:41:12.968102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743036072.981101 1998765 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743036072.984812 1998765 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-26 19:41:12.999887: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from re import match\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_plays = '/home/research/Big Data Bowl/raw_data_folders/nfl-big-data-bowl-2025/plays.csv'\n",
    "play_data = pd.read_csv(csv_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clm in ['dropbackType','rushLocationType','pff_runConceptPrimary','pff_passCoverage','pff_manZone']:\n",
    "    play_data[clm] = np.where(play_data['qbKneel'] == 1, 'QB_KNEEL', play_data[clm])\n",
    "    play_data[clm] = np.where(play_data['qbSpike'] == True, 'QB_SPIKE', play_data[clm])\n",
    "play_data['offenseFormation'] = np.where((play_data['offenseFormation'].isna()) & (play_data['qbKneel'] == 1), 'QB_KNEEL', play_data['offenseFormation'])\n",
    "play_data['offenseFormation'] = np.where((play_data['offenseFormation'].isna()) & (play_data['qbSpike'] == True), 'QB_SPIKE', play_data['offenseFormation'])\n",
    "play_data['dropbackType'] = np.where(play_data['qbSneak'] == True, 'QB_SNEAK', play_data['dropbackType'])\n",
    "play_data['dropbackType'] = np.where((play_data['qbKneel'] == 0) & (play_data['qbSneak'] == False) & ((play_data['dropbackType'].isna()) | (play_data['dropbackType'] == \"UNKNOWN\")), 'NON_QB_RUN', play_data['dropbackType'])\n",
    "play_data['playType'] = np.where(play_data['rushLocationType'].isna(), play_data['dropbackType'].astype(str), play_data['dropbackType'].astype(str) + '/' + play_data['rushLocationType'].astype(str))\n",
    "play_data['receiverAlignment'] = np.where(play_data['receiverAlignment'].isna(), '0x0', play_data['receiverAlignment'].astype(str))\n",
    "play_data['playAction'] = np.where(play_data['playAction'] == True, 1, 0)\n",
    "play_data['pff_passCoverage'] = np.where((play_data['qbSneak'] == True) & (play_data['pff_passCoverage'].isna()), 'QB_SNEAK', play_data['pff_passCoverage'])\n",
    "play_data = play_data[play_data['pff_passCoverage'] != 'Miscellaneous']\n",
    "play_data = play_data[play_data['pff_passCoverage'].notna()]\n",
    "play_data['pff_runConceptPrimary'] = np.where(((play_data['pff_runConceptPrimary'].isna()) | (play_data['pff_runConceptPrimary'] == 'UNDEFINED')) & (play_data['dropbackType'] != 'UNKNOWN') & (play_data['dropbackType'].notna()), 'DESIGNED_PASS', play_data['pff_runConceptPrimary'])\n",
    "play_data['runConcept'] = np.where(play_data['pff_runConceptSecondary'].isna(), play_data['pff_runConceptPrimary'].astype(str), play_data['pff_runConceptPrimary'].astype(str) + ';' + play_data['pff_runConceptSecondary'].astype(str))\n",
    "play_data['offensiveSuccess'] = np.where(((play_data['down'] == 1) | (play_data['down'] == 2)) & (play_data['prePenaltyYardsGained'] >= play_data['yardsToGo']/2), 1, 0)\n",
    "play_data['offensiveSuccess'] = np.where(((play_data['down'] == 3) | (play_data['down'] == 4)) & (play_data['prePenaltyYardsGained'] >= play_data['yardsToGo']), 1, 0)\n",
    "play_data['game_play'] = play_data['gameId'].astype(str) + '_' + play_data['playId'].astype(str)\n",
    "play_data2 = play_data[['game_play','playAction','pff_runPassOption','offenseFormation','receiverAlignment','playType','pff_passCoverage','runConcept','offensiveSuccess']]\n",
    "play_data3 = play_data2.rename(columns={'playAction': 'isPA', 'pff_runPassOption': 'isRPO', 'offenseFormation': 'off_form', 'receiverAlignment': 'rec_form','pff_passCoverage': 'passCov', 'offensiveSuccess': 'off_scs'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_of = {value: index for index, value in enumerate(list(np.unique(play_data3['off_form'])))}\n",
    "labels_rf = {value: index for index, value in enumerate(list(np.unique(play_data3['rec_form'])))}\n",
    "labels_pt = {value: index for index, value in enumerate(list(np.unique(play_data3['playType'])))}\n",
    "labels_pc = {value: index for index, value in enumerate(list(np.unique(play_data3['passCov'])))}\n",
    "labels_rc = {value: index for index, value in enumerate(list(np.unique(play_data3['runConcept'])))}\n",
    "play_data3['of_norm'] = play_data3['off_form'].map(labels_of)\n",
    "play_data3['rf_norm'] = play_data3['rec_form'].map(labels_rf)\n",
    "play_data3['pt_norm'] = play_data3['playType'].map(labels_pt)\n",
    "play_data3['pc_norm'] = play_data3['passCov'].map(labels_pc)\n",
    "play_data3['rc_norm'] = play_data3['runConcept'].map(labels_rc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_csv('/home/research/Big Data Bowl/raw_data_folders/nfl-big-data-bowl-2025/players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_plays = pd.DataFrame()\n",
    "for wk in np.arange(9)+1:\n",
    "    gp = pd.read_csv('/home/research/Big Data Bowl/raw_data_folders/nfl-big-data-bowl-2025/tracking_week_' + str(wk) + '.csv')\n",
    "    game_plays = pd.concat([game_plays, gp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_plays['game_play'] = game_plays['gameId'].astype(str) + '_' + game_plays['playId'].astype(str)\n",
    "gameplays0 = pd.merge(game_plays, players[['nflId','position']], how='left', on='nflId')\n",
    "gameplays1 = pd.merge(gameplays0, play_data[['game_play','possessionTeam','yardlineSide','yardlineNumber','yardsToGo']], how='left', on='game_play')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assess the number of frames before and after snap\n",
    "gpx = gameplays1[gameplays1['displayName'] == 'football'].groupby(['game_play','frameType']).size()\n",
    "# gpxw = pd.pivot_table(pd.DataFrame(gpx), index='game_play', columns='frameType')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameplays2 = gameplays1[['game_play','frameId','position','playDirection','x','y','club','possessionTeam','displayName']]\n",
    "gameplays2 = gameplays2.rename(columns={'x': 'x_pos', 'y': 'y_pos'})\n",
    "gameplays2.loc[(gameplays2['playDirection'] == 'left'), 'x_pos'] = 120 - gameplays2['x_pos']\n",
    "gameplays2.loc[(gameplays2['playDirection'] == 'left'), 'y_pos'] = (160/3) - gameplays2['y_pos']\n",
    "# gameplays2['side'] = np.where(gameplays2['club'] == gameplays2['possessionTeam'], 'QB_SNEAK', play_data['pff_passCoverage'])\n",
    "gameplays2['side'] = 'other'\n",
    "gameplays2.loc[(gameplays2['club'] == gameplays2['possessionTeam']) & (gameplays2['displayName'] != 'football'), 'side'] = 'off'\n",
    "gameplays2.loc[(gameplays2['club'] != gameplays2['possessionTeam']) & (gameplays2['displayName'] != 'football'), 'side'] = 'def'\n",
    "gameplays2.loc[gameplays2['displayName'] == 'football','side'] = 'fb'\n",
    "gameplays2.loc[gameplays2['side'] == 'fb', 'position'] = 'FTBL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove plays with offensive and defensive position mismatches (e.g., DT on offense)\n",
    "mismatch_off = gameplays2.loc[(gameplays2['position'].isin(['G','T','TE','C','WR','QB','RB','FB'])) & (gameplays2['side'] == 'def'),'game_play'].unique()\n",
    "mismatch_def = gameplays2.loc[(gameplays2['position'].isin(['OLB','DT','ILB','FS'])) & (gameplays2['side'] == 'off'),'game_play'].unique()\n",
    "mismatch_tot = mismatch_def.tolist() + mismatch_off.tolist()\n",
    "gameplays3 = gameplays2.drop(gameplays2.loc[gameplays2[\"game_play\"].isin(mismatch_tot)].index)\n",
    "pos_groups = gameplays3.groupby('side')['position'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_codes = {'QB':2, 'G':3, 'WR':4, 'C':5, 'RB':6, 'TE':7, 'T':8, 'FB':9, \n",
    "                'SS':-2, 'CB':-3, 'OLB':-4, 'DE':-5, 'ILB':-6, 'FS':-7, 'NT':-8, \n",
    "                'DT':-9, 'MLB':-10, 'LB':-11, 'DB':-12, 'FTBL':50}\n",
    "gameplays3['pos_id'] = gameplays3['position'].map(pos_codes)\n",
    "gameplays3['col_idx'] = round(100*gameplays3['x_pos']/120).astype(int)\n",
    "gameplays3['row_idx'] = round(100*gameplays3['y_pos']/(160/3)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameplays3 = pd.merge(gameplays3, gameplays3[(gameplays3['position'] == 'FTBL') & (gameplays3['frameId'] == 1)][['game_play','col_idx']], how='left', on='game_play')\n",
    "gameplays3['col_norm'] = gameplays3['col_idx_x'] + (50 - gameplays3['col_idx_y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only relevant features\n",
    "gameplays3 = gameplays3[['game_play', \"frameId\", 'pos_id', 'row_idx', 'col_norm']]\n",
    "under_thresh = gameplays3.groupby('game_play')['frameId'].nunique()\n",
    "thresh_ids = under_thresh[under_thresh <= 350].index.to_list()     \n",
    "gameplays4 = gameplays3[gameplays3['game_play'].isin(thresh_ids)]\n",
    "play_data4 = play_data3[play_data3['game_play'].isin(thresh_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16059\n",
      "16059\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(gameplays4['game_play'])))\n",
    "print(len(np.unique(play_data4['game_play'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking data saved!\n",
      "Label data saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train and test data\n",
    "trainplays, testplays = train_test_split(\n",
    "    np.unique(play_data4['game_play']), \n",
    "    train_size=0.8, \n",
    "    random_state=1, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create a single folder to store all files\n",
    "save_dir = \"splits\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save tracking data\n",
    "gameplays4[gameplays4['game_play'].isin(trainplays)].to_pickle(os.path.join(save_dir, 'tracking_processed_train.pkl'))\n",
    "gameplays4[gameplays4['game_play'].isin(testplays)].to_pickle(os.path.join(save_dir, 'tracking_processed_test.pkl'))\n",
    "print('Tracking data saved!')\n",
    "\n",
    "# Save label data\n",
    "play_data4[play_data4['game_play'].isin(trainplays)].to_pickle(os.path.join(save_dir, 'label_processed_train.pkl'))\n",
    "play_data4[play_data4['game_play'].isin(testplays)].to_pickle(os.path.join(save_dir, 'label_processed_test.pkl'))\n",
    "print('Label data saved!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files were made successfully and they were saved in train test splits.\n"
     ]
    }
   ],
   "source": [
    "print(\"The files were made successfully and they were saved in train test splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
