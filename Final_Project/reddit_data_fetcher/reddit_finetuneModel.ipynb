{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oDZbpZReFFv"
   },
   "outputs": [],
   "source": [
    "# !pip install praw\n",
    "# !pip install faiss-cpu transformers sentence-transformers datasets torch\n",
    "# !pip install dask\n",
    "# !pip install dask[dataframe]\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install qdrant-client sentence-transformers pandas\n",
    "# !pip install swifter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from prawcore.exceptions import Redirect\n",
    "import datetime\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"clientid\",  # Your client ID like biWPqknyVXfp a4gvym_peA\n",
    "    client_secret=\"clientsecret\",  # Your client secret N5e9zLgIiOqc UqYgChbVfrI8XUIqmA\n",
    "    user_agent=\"Common_Salt2614\"  # Replace with your Reddit username\n",
    ")\n",
    "\n",
    "def generate_search_queries():\n",
    "    nfl_teams = [\n",
    "        \"Baltimore Ravens\",\n",
    "        \"Cincinnati Bengals\",\n",
    "        \"Cleveland Browns\",\n",
    "        \"Pittsburgh Steelers\",\n",
    "        \"Houston Texans\",\n",
    "        \"Indianapolis Colts\",\n",
    "        \"Jacksonville Jaguars\",\n",
    "        \"Tennessee Titans\",\n",
    "        \"Buffalo Bills\",\n",
    "        \"Miami Dolphins\",\n",
    "        \"New England Patriots\",\n",
    "        \"New York Jets\",\n",
    "        \"Denver Broncos\",\n",
    "        \"Kansas City Chiefs\",\n",
    "        \"Las Vegas Raiders\",\n",
    "        \"Los Angeles Chargers\",\n",
    "        \"Chicago Bears\",\n",
    "        \"Detroit Lions\",\n",
    "        \"Green Bay Packers\",\n",
    "        \"Minnesota Vikings\",\n",
    "        \"Atlanta Falcons\",\n",
    "        \"Carolina Panthers\",\n",
    "        \"New Orleans Saints\",\n",
    "        \"Tampa Bay Buccaneers\",\n",
    "        \"Dallas Cowboys\",\n",
    "        \"New York Giants\",\n",
    "        \"Philadelphia Eagles\",\n",
    "        \"Washington Commanders\",\n",
    "        \"Arizona Cardinals\",\n",
    "        \"Los Angeles Rams\",\n",
    "        \"San Francisco 49ers\",\n",
    "        \"Seattle Seahawks\",\n",
    "    ]  # Your team list\n",
    "    players = [\n",
    "        \"Patrick Mahomes\",\n",
    "        \"Justin Jefferson\",\n",
    "        \"Nick Bosa\",\n",
    "        \"Travis Kelce\",\n",
    "        \"Tua Tagovailoa\",\n",
    "        \"Micah Parsons\",\n",
    "        \"Tyreek Hill\",\n",
    "        \"Joe Burrow\",\n",
    "        \"Christian McCaffrey\",\n",
    "        \"Aaron Donald\",\n",
    "        \"Josh Allen\",\n",
    "        \"Lamar Jackson\",\n",
    "        \"Ja'Marr Chase\",\n",
    "        \"T.J. Watt\",\n",
    "        \"Cooper Kupp\",\n",
    "        \"Dak Prescott\",\n",
    "        \"Derick Henry\",\n",
    "        \"Sauce Gardner\",\n",
    "        \"Jalen Hurts\",\n",
    "        \"Myles Garrett\",\n",
    "        \"Trevor Lawrence\",\n",
    "        \"A.J. Brown\",\n",
    "        \"Fred Warner\",\n",
    "        \"Justin Herbert\",\n",
    "        \"Bijan Robinson\",\n",
    "        \"Dexter Lawrence\",\n",
    "        \"CeeDee Lamb\",\n",
    "        \"Trent Williams\",\n",
    "        \"Minkah Fitzpatrick\",\n",
    "        \"Ammon-Ra St. Brown\",\n",
    "    ]  # Removed positions\n",
    "    keywords_win_loss = [\"win\", \"loss\", \"defeated\", \"upset\"]\n",
    "    keywords_performance = [\"performance\", \"MVP\", \"breakout\", \"struggling\"]  # shortened keywords.\n",
    "    keywords_strengths_weaknesses = [\"strength\", \"weakness\", \"passing\", \"rushing\"]  # shortened keywords.\n",
    "    keywords_off_def = [\"offense\", \"defense\"]  # shortened keywords.\n",
    "\n",
    "    queries = []\n",
    "    for team in nfl_teams:\n",
    "        for keyword in keywords_win_loss:\n",
    "            queries.append(f\"{team} {keyword}\")\n",
    "        for keyword in keywords_off_def:\n",
    "            queries.append(f\"{team} {keyword}\")\n",
    "\n",
    "    for player in players:\n",
    "        for keyword in keywords_performance:\n",
    "            queries.append(f\"{player} {keyword}\")\n",
    "        for keyword in keywords_strengths_weaknesses:\n",
    "            queries.append(f\"{player} {keyword}\")\n",
    "\n",
    "    return queries\n",
    "\n",
    "search_queries = generate_search_queries()\n",
    "target_subreddits = [\n",
    "    \"nfl\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tasks to process: 432\n"
     ]
    }
   ],
   "source": [
    "# Build task list - all combinations of subreddits and queries\n",
    "tasks = [(sub, query) for sub in target_subreddits for query in search_queries]\n",
    "print(f\"Total tasks to process: {len(tasks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  25%|██▌       | 110/432 [00:00<00:00, 547.04it/s, Current: nfl/Green Bay Packers lo]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping nfl/Baltimore Ravens win - contains 100 JSON files\n",
      "Completed nfl/Baltimore Ravens win: 0 posts in 0.0s\n",
      "Skipping nfl/Baltimore Ravens loss - contains 100 JSON files\n",
      "Completed nfl/Baltimore Ravens loss: 0 posts in 0.0s\n",
      "Skipping nfl/Baltimore Ravens defeated - contains 100 JSON files\n",
      "Completed nfl/Baltimore Ravens defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Baltimore Ravens upset - contains 100 JSON files\n",
      "Completed nfl/Baltimore Ravens upset: 0 posts in 0.0s\n",
      "Skipping nfl/Baltimore Ravens offense - contains 100 JSON files\n",
      "Completed nfl/Baltimore Ravens offense: 0 posts in 0.0s\n",
      "Skipping nfl/Baltimore Ravens defense - contains 100 JSON files\n",
      "Completed nfl/Baltimore Ravens defense: 0 posts in 0.0s\n",
      "Skipping nfl/Cincinnati Bengals win - contains 100 JSON files\n",
      "Completed nfl/Cincinnati Bengals win: 0 posts in 0.0s\n",
      "Skipping nfl/Cincinnati Bengals loss - contains 100 JSON files\n",
      "Completed nfl/Cincinnati Bengals loss: 0 posts in 0.0s\n",
      "Skipping nfl/Cincinnati Bengals defeated - contains 100 JSON files\n",
      "Completed nfl/Cincinnati Bengals defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Cincinnati Bengals upset - contains 100 JSON files\n",
      "Completed nfl/Cincinnati Bengals upset: 0 posts in 0.0s\n",
      "Skipping nfl/Cincinnati Bengals offense - contains 100 JSON files\n",
      "Completed nfl/Cincinnati Bengals offense: 0 posts in 0.0s\n",
      "Skipping nfl/Cincinnati Bengals defense - contains 100 JSON files\n",
      "Completed nfl/Cincinnati Bengals defense: 0 posts in 0.0s\n",
      "Skipping nfl/Cleveland Browns win - contains 100 JSON files\n",
      "Completed nfl/Cleveland Browns win: 0 posts in 0.0s\n",
      "Skipping nfl/Cleveland Browns loss - contains 100 JSON files\n",
      "Completed nfl/Cleveland Browns loss: 0 posts in 0.0s\n",
      "Skipping nfl/Cleveland Browns defeated - contains 100 JSON files\n",
      "Completed nfl/Cleveland Browns defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Cleveland Browns upset - contains 100 JSON files\n",
      "Completed nfl/Cleveland Browns upset: 0 posts in 0.0s\n",
      "Skipping nfl/Cleveland Browns offense - contains 100 JSON files\n",
      "Completed nfl/Cleveland Browns offense: 0 posts in 0.0s\n",
      "Skipping nfl/Cleveland Browns defense - contains 100 JSON files\n",
      "Completed nfl/Cleveland Browns defense: 0 posts in 0.0s\n",
      "Skipping nfl/Pittsburgh Steelers win - contains 100 JSON files\n",
      "Completed nfl/Pittsburgh Steelers win: 0 posts in 0.0s\n",
      "Skipping nfl/Pittsburgh Steelers loss - contains 100 JSON files\n",
      "Completed nfl/Pittsburgh Steelers loss: 0 posts in 0.0s\n",
      "Skipping nfl/Pittsburgh Steelers defeated - contains 100 JSON files\n",
      "Completed nfl/Pittsburgh Steelers defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Pittsburgh Steelers upset - contains 100 JSON files\n",
      "Completed nfl/Pittsburgh Steelers upset: 0 posts in 0.0s\n",
      "Skipping nfl/Pittsburgh Steelers offense - contains 100 JSON files\n",
      "Completed nfl/Pittsburgh Steelers offense: 0 posts in 0.0s\n",
      "Skipping nfl/Pittsburgh Steelers defense - contains 100 JSON files\n",
      "Completed nfl/Pittsburgh Steelers defense: 0 posts in 0.0s\n",
      "Skipping nfl/Houston Texans win - contains 100 JSON files\n",
      "Completed nfl/Houston Texans win: 0 posts in 0.0s\n",
      "Skipping nfl/Houston Texans loss - contains 100 JSON files\n",
      "Completed nfl/Houston Texans loss: 0 posts in 0.0s\n",
      "Skipping nfl/Houston Texans defeated - contains 100 JSON files\n",
      "Completed nfl/Houston Texans defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Houston Texans upset - contains 100 JSON files\n",
      "Completed nfl/Houston Texans upset: 0 posts in 0.0s\n",
      "Skipping nfl/Houston Texans offense - contains 100 JSON files\n",
      "Completed nfl/Houston Texans offense: 0 posts in 0.0s\n",
      "Skipping nfl/Houston Texans defense - contains 100 JSON files\n",
      "Completed nfl/Houston Texans defense: 0 posts in 0.0s\n",
      "Skipping nfl/Indianapolis Colts win - contains 100 JSON files\n",
      "Completed nfl/Indianapolis Colts win: 0 posts in 0.0s\n",
      "Skipping nfl/Indianapolis Colts loss - contains 100 JSON files\n",
      "Completed nfl/Indianapolis Colts loss: 0 posts in 0.0s\n",
      "Skipping nfl/Indianapolis Colts defeated - contains 100 JSON files\n",
      "Completed nfl/Indianapolis Colts defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Indianapolis Colts upset - contains 100 JSON files\n",
      "Completed nfl/Indianapolis Colts upset: 0 posts in 0.0s\n",
      "Skipping nfl/Indianapolis Colts offense - contains 100 JSON files\n",
      "Completed nfl/Indianapolis Colts offense: 0 posts in 0.0s\n",
      "Skipping nfl/Indianapolis Colts defense - contains 38 JSON files\n",
      "Completed nfl/Indianapolis Colts defense: 0 posts in 0.0s\n",
      "Skipping nfl/Jacksonville Jaguars win - contains 100 JSON files\n",
      "Completed nfl/Jacksonville Jaguars win: 0 posts in 0.0s\n",
      "Skipping nfl/Jacksonville Jaguars loss - contains 100 JSON files\n",
      "Completed nfl/Jacksonville Jaguars loss: 0 posts in 0.0s\n",
      "Skipping nfl/Jacksonville Jaguars defeated - contains 100 JSON files\n",
      "Completed nfl/Jacksonville Jaguars defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Jacksonville Jaguars upset - contains 100 JSON files\n",
      "Completed nfl/Jacksonville Jaguars upset: 0 posts in 0.0s\n",
      "Skipping nfl/Jacksonville Jaguars offense - contains 100 JSON files\n",
      "Completed nfl/Jacksonville Jaguars offense: 0 posts in 0.0s\n",
      "Skipping nfl/Jacksonville Jaguars defense - contains 100 JSON files\n",
      "Completed nfl/Jacksonville Jaguars defense: 0 posts in 0.0s\n",
      "Skipping nfl/Tennessee Titans win - contains 100 JSON files\n",
      "Completed nfl/Tennessee Titans win: 0 posts in 0.0s\n",
      "Skipping nfl/Tennessee Titans loss - contains 100 JSON files\n",
      "Completed nfl/Tennessee Titans loss: 0 posts in 0.0s\n",
      "Skipping nfl/Tennessee Titans defeated - contains 100 JSON files\n",
      "Completed nfl/Tennessee Titans defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Tennessee Titans upset - contains 100 JSON files\n",
      "Completed nfl/Tennessee Titans upset: 0 posts in 0.0s\n",
      "Skipping nfl/Tennessee Titans offense - contains 100 JSON files\n",
      "Completed nfl/Tennessee Titans offense: 0 posts in 0.0s\n",
      "Skipping nfl/Tennessee Titans defense - contains 100 JSON files\n",
      "Completed nfl/Tennessee Titans defense: 0 posts in 0.0s\n",
      "Skipping nfl/Buffalo Bills win - contains 100 JSON files\n",
      "Completed nfl/Buffalo Bills win: 0 posts in 0.0s\n",
      "Skipping nfl/Buffalo Bills loss - contains 100 JSON files\n",
      "Completed nfl/Buffalo Bills loss: 0 posts in 0.0s\n",
      "Skipping nfl/Buffalo Bills defeated - contains 100 JSON files\n",
      "Completed nfl/Buffalo Bills defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Buffalo Bills upset - contains 100 JSON files\n",
      "Completed nfl/Buffalo Bills upset: 0 posts in 0.0s\n",
      "Skipping nfl/Buffalo Bills offense - contains 100 JSON files\n",
      "Completed nfl/Buffalo Bills offense: 0 posts in 0.0s\n",
      "Skipping nfl/Buffalo Bills defense - contains 100 JSON files\n",
      "Completed nfl/Buffalo Bills defense: 0 posts in 0.0s\n",
      "Skipping nfl/Miami Dolphins win - contains 100 JSON files\n",
      "Completed nfl/Miami Dolphins win: 0 posts in 0.0s\n",
      "Skipping nfl/Miami Dolphins loss - contains 100 JSON files\n",
      "Completed nfl/Miami Dolphins loss: 0 posts in 0.0s\n",
      "Skipping nfl/Miami Dolphins defeated - contains 100 JSON files\n",
      "Completed nfl/Miami Dolphins defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Miami Dolphins upset - contains 100 JSON files\n",
      "Completed nfl/Miami Dolphins upset: 0 posts in 0.0s\n",
      "Skipping nfl/Miami Dolphins offense - contains 100 JSON files\n",
      "Completed nfl/Miami Dolphins offense: 0 posts in 0.0s\n",
      "Skipping nfl/Miami Dolphins defense - contains 100 JSON files\n",
      "Completed nfl/Miami Dolphins defense: 0 posts in 0.0s\n",
      "Skipping nfl/New England Patriots win - contains 100 JSON files\n",
      "Completed nfl/New England Patriots win: 0 posts in 0.0s\n",
      "Skipping nfl/New England Patriots loss - contains 100 JSON files\n",
      "Completed nfl/New England Patriots loss: 0 posts in 0.0s\n",
      "Skipping nfl/New England Patriots defeated - contains 100 JSON files\n",
      "Completed nfl/New England Patriots defeated: 0 posts in 0.0s\n",
      "Skipping nfl/New England Patriots upset - contains 100 JSON files\n",
      "Completed nfl/New England Patriots upset: 0 posts in 0.0s\n",
      "Skipping nfl/New England Patriots offense - contains 100 JSON files\n",
      "Completed nfl/New England Patriots offense: 0 posts in 0.0s\n",
      "Skipping nfl/New England Patriots defense - contains 100 JSON files\n",
      "Completed nfl/New England Patriots defense: 0 posts in 0.0s\n",
      "Skipping nfl/New York Jets win - contains 100 JSON files\n",
      "Completed nfl/New York Jets win: 0 posts in 0.0s\n",
      "Skipping nfl/New York Jets loss - contains 100 JSON files\n",
      "Completed nfl/New York Jets loss: 0 posts in 0.0s\n",
      "Skipping nfl/New York Jets defeated - contains 100 JSON files\n",
      "Completed nfl/New York Jets defeated: 0 posts in 0.0s\n",
      "Skipping nfl/New York Jets upset - contains 100 JSON files\n",
      "Completed nfl/New York Jets upset: 0 posts in 0.0s\n",
      "Skipping nfl/New York Jets offense - contains 100 JSON files\n",
      "Completed nfl/New York Jets offense: 0 posts in 0.0s\n",
      "Skipping nfl/New York Jets defense - contains 100 JSON files\n",
      "Completed nfl/New York Jets defense: 0 posts in 0.0s\n",
      "Skipping nfl/Denver Broncos win - contains 35 JSON files\n",
      "Completed nfl/Denver Broncos win: 0 posts in 0.0s\n",
      "Skipping nfl/Denver Broncos loss - contains 100 JSON files\n",
      "Completed nfl/Denver Broncos loss: 0 posts in 0.0s\n",
      "Skipping nfl/Denver Broncos defeated - contains 100 JSON files\n",
      "Completed nfl/Denver Broncos defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Denver Broncos upset - contains 100 JSON files\n",
      "Completed nfl/Denver Broncos upset: 0 posts in 0.0s\n",
      "Skipping nfl/Denver Broncos offense - contains 100 JSON files\n",
      "Completed nfl/Denver Broncos offense: 0 posts in 0.0s\n",
      "Skipping nfl/Denver Broncos defense - contains 100 JSON files\n",
      "Completed nfl/Denver Broncos defense: 0 posts in 0.0s\n",
      "Skipping nfl/Kansas City Chiefs win - contains 100 JSON files\n",
      "Completed nfl/Kansas City Chiefs win: 0 posts in 0.0s\n",
      "Skipping nfl/Kansas City Chiefs loss - contains 100 JSON files\n",
      "Completed nfl/Kansas City Chiefs loss: 0 posts in 0.0s\n",
      "Skipping nfl/Kansas City Chiefs defeated - contains 100 JSON files\n",
      "Completed nfl/Kansas City Chiefs defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Kansas City Chiefs upset - contains 100 JSON files\n",
      "Completed nfl/Kansas City Chiefs upset: 0 posts in 0.0s\n",
      "Skipping nfl/Kansas City Chiefs offense - contains 100 JSON files\n",
      "Completed nfl/Kansas City Chiefs offense: 0 posts in 0.0s\n",
      "Skipping nfl/Kansas City Chiefs defense - contains 100 JSON files\n",
      "Completed nfl/Kansas City Chiefs defense: 0 posts in 0.0s\n",
      "Skipping nfl/Las Vegas Raiders win - contains 100 JSON files\n",
      "Completed nfl/Las Vegas Raiders win: 0 posts in 0.0s\n",
      "Skipping nfl/Las Vegas Raiders loss - contains 100 JSON files\n",
      "Completed nfl/Las Vegas Raiders loss: 0 posts in 0.0s\n",
      "Skipping nfl/Las Vegas Raiders defeated - contains 100 JSON files\n",
      "Completed nfl/Las Vegas Raiders defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Las Vegas Raiders upset - contains 100 JSON files\n",
      "Completed nfl/Las Vegas Raiders upset: 0 posts in 0.0s\n",
      "Skipping nfl/Las Vegas Raiders offense - contains 100 JSON files\n",
      "Completed nfl/Las Vegas Raiders offense: 0 posts in 0.0s\n",
      "Skipping nfl/Las Vegas Raiders defense - contains 100 JSON files\n",
      "Completed nfl/Las Vegas Raiders defense: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Chargers win - contains 100 JSON files\n",
      "Completed nfl/Los Angeles Chargers win: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Chargers loss - contains 100 JSON files\n",
      "Completed nfl/Los Angeles Chargers loss: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Chargers defeated - contains 100 JSON files\n",
      "Completed nfl/Los Angeles Chargers defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Chargers upset - contains 100 JSON files\n",
      "Completed nfl/Los Angeles Chargers upset: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Chargers offense - contains 90 JSON files\n",
      "Completed nfl/Los Angeles Chargers offense: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Chargers defense - contains 100 JSON files\n",
      "Completed nfl/Los Angeles Chargers defense: 0 posts in 0.0s\n",
      "Skipping nfl/Chicago Bears win - contains 100 JSON files\n",
      "Completed nfl/Chicago Bears win: 0 posts in 0.0s\n",
      "Skipping nfl/Chicago Bears loss - contains 100 JSON files\n",
      "Completed nfl/Chicago Bears loss: 0 posts in 0.0s\n",
      "Skipping nfl/Chicago Bears defeated - contains 100 JSON files\n",
      "Completed nfl/Chicago Bears defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Chicago Bears upset - contains 100 JSON files\n",
      "Completed nfl/Chicago Bears upset: 0 posts in 0.0s\n",
      "Skipping nfl/Chicago Bears offense - contains 100 JSON files\n",
      "Completed nfl/Chicago Bears offense: 0 posts in 0.0s\n",
      "Skipping nfl/Chicago Bears defense - contains 100 JSON files\n",
      "Completed nfl/Chicago Bears defense: 0 posts in 0.0s\n",
      "Skipping nfl/Detroit Lions win - contains 100 JSON files\n",
      "Completed nfl/Detroit Lions win: 0 posts in 0.0s\n",
      "Skipping nfl/Detroit Lions loss - contains 100 JSON files\n",
      "Completed nfl/Detroit Lions loss: 0 posts in 0.0s\n",
      "Skipping nfl/Detroit Lions defeated - contains 100 JSON files\n",
      "Completed nfl/Detroit Lions defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Detroit Lions upset - contains 100 JSON files\n",
      "Completed nfl/Detroit Lions upset: 0 posts in 0.0s\n",
      "Skipping nfl/Detroit Lions offense - contains 100 JSON files\n",
      "Completed nfl/Detroit Lions offense: 0 posts in 0.0s\n",
      "Skipping nfl/Detroit Lions defense - contains 100 JSON files\n",
      "Completed nfl/Detroit Lions defense: 0 posts in 0.0s\n",
      "Skipping nfl/Green Bay Packers win - contains 100 JSON files\n",
      "Completed nfl/Green Bay Packers win: 0 posts in 0.0s\n",
      "Skipping nfl/Green Bay Packers loss - contains 100 JSON files\n",
      "Completed nfl/Green Bay Packers loss: 0 posts in 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  38%|███▊      | 165/432 [00:00<00:00, 539.84it/s, Current: nfl/Travis Kelce perform]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping nfl/Green Bay Packers defeated - contains 100 JSON files\n",
      "Completed nfl/Green Bay Packers defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Green Bay Packers upset - contains 100 JSON files\n",
      "Completed nfl/Green Bay Packers upset: 0 posts in 0.0s\n",
      "Skipping nfl/Green Bay Packers offense - contains 100 JSON files\n",
      "Completed nfl/Green Bay Packers offense: 0 posts in 0.0s\n",
      "Skipping nfl/Green Bay Packers defense - contains 100 JSON files\n",
      "Completed nfl/Green Bay Packers defense: 0 posts in 0.0s\n",
      "Skipping nfl/Minnesota Vikings win - contains 100 JSON files\n",
      "Completed nfl/Minnesota Vikings win: 0 posts in 0.0s\n",
      "Skipping nfl/Minnesota Vikings loss - contains 100 JSON files\n",
      "Completed nfl/Minnesota Vikings loss: 0 posts in 0.0s\n",
      "Skipping nfl/Minnesota Vikings defeated - contains 100 JSON files\n",
      "Completed nfl/Minnesota Vikings defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Minnesota Vikings upset - contains 100 JSON files\n",
      "Completed nfl/Minnesota Vikings upset: 0 posts in 0.0s\n",
      "Skipping nfl/Minnesota Vikings offense - contains 100 JSON files\n",
      "Completed nfl/Minnesota Vikings offense: 0 posts in 0.0s\n",
      "Skipping nfl/Minnesota Vikings defense - contains 100 JSON files\n",
      "Completed nfl/Minnesota Vikings defense: 0 posts in 0.0s\n",
      "Skipping nfl/Atlanta Falcons win - contains 100 JSON files\n",
      "Completed nfl/Atlanta Falcons win: 0 posts in 0.0s\n",
      "Skipping nfl/Atlanta Falcons loss - contains 100 JSON files\n",
      "Completed nfl/Atlanta Falcons loss: 0 posts in 0.0s\n",
      "Skipping nfl/Atlanta Falcons defeated - contains 100 JSON files\n",
      "Completed nfl/Atlanta Falcons defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Atlanta Falcons upset - contains 100 JSON files\n",
      "Completed nfl/Atlanta Falcons upset: 0 posts in 0.0s\n",
      "Skipping nfl/Atlanta Falcons offense - contains 100 JSON files\n",
      "Completed nfl/Atlanta Falcons offense: 0 posts in 0.0s\n",
      "Skipping nfl/Atlanta Falcons defense - contains 100 JSON files\n",
      "Completed nfl/Atlanta Falcons defense: 0 posts in 0.0s\n",
      "Skipping nfl/Carolina Panthers win - contains 100 JSON files\n",
      "Completed nfl/Carolina Panthers win: 0 posts in 0.0s\n",
      "Skipping nfl/Carolina Panthers loss - contains 100 JSON files\n",
      "Completed nfl/Carolina Panthers loss: 0 posts in 0.0s\n",
      "Skipping nfl/Carolina Panthers defeated - contains 100 JSON files\n",
      "Completed nfl/Carolina Panthers defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Carolina Panthers upset - contains 100 JSON files\n",
      "Completed nfl/Carolina Panthers upset: 0 posts in 0.0s\n",
      "Skipping nfl/Carolina Panthers offense - contains 100 JSON files\n",
      "Completed nfl/Carolina Panthers offense: 0 posts in 0.0s\n",
      "Skipping nfl/Carolina Panthers defense - contains 100 JSON files\n",
      "Completed nfl/Carolina Panthers defense: 0 posts in 0.0s\n",
      "Skipping nfl/New Orleans Saints win - contains 100 JSON files\n",
      "Completed nfl/New Orleans Saints win: 0 posts in 0.0s\n",
      "Skipping nfl/New Orleans Saints loss - contains 100 JSON files\n",
      "Completed nfl/New Orleans Saints loss: 0 posts in 0.0s\n",
      "Skipping nfl/New Orleans Saints defeated - contains 100 JSON files\n",
      "Completed nfl/New Orleans Saints defeated: 0 posts in 0.0s\n",
      "Skipping nfl/New Orleans Saints upset - contains 100 JSON files\n",
      "Completed nfl/New Orleans Saints upset: 0 posts in 0.0s\n",
      "Skipping nfl/New Orleans Saints offense - contains 100 JSON files\n",
      "Completed nfl/New Orleans Saints offense: 0 posts in 0.0s\n",
      "Skipping nfl/New Orleans Saints defense - contains 100 JSON files\n",
      "Completed nfl/New Orleans Saints defense: 0 posts in 0.0s\n",
      "Skipping nfl/Tampa Bay Buccaneers win - contains 100 JSON files\n",
      "Completed nfl/Tampa Bay Buccaneers win: 0 posts in 0.0s\n",
      "Skipping nfl/Tampa Bay Buccaneers loss - contains 100 JSON files\n",
      "Completed nfl/Tampa Bay Buccaneers loss: 0 posts in 0.0s\n",
      "Skipping nfl/Tampa Bay Buccaneers defeated - contains 100 JSON files\n",
      "Completed nfl/Tampa Bay Buccaneers defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Tampa Bay Buccaneers upset - contains 100 JSON files\n",
      "Completed nfl/Tampa Bay Buccaneers upset: 0 posts in 0.0s\n",
      "Skipping nfl/Tampa Bay Buccaneers offense - contains 100 JSON files\n",
      "Completed nfl/Tampa Bay Buccaneers offense: 0 posts in 0.0s\n",
      "Skipping nfl/Tampa Bay Buccaneers defense - contains 100 JSON files\n",
      "Completed nfl/Tampa Bay Buccaneers defense: 0 posts in 0.0s\n",
      "Skipping nfl/Dallas Cowboys win - contains 100 JSON files\n",
      "Completed nfl/Dallas Cowboys win: 0 posts in 0.0s\n",
      "Skipping nfl/Dallas Cowboys loss - contains 100 JSON files\n",
      "Completed nfl/Dallas Cowboys loss: 0 posts in 0.0s\n",
      "Skipping nfl/Dallas Cowboys defeated - contains 100 JSON files\n",
      "Completed nfl/Dallas Cowboys defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Dallas Cowboys upset - contains 100 JSON files\n",
      "Completed nfl/Dallas Cowboys upset: 0 posts in 0.0s\n",
      "Skipping nfl/Dallas Cowboys offense - contains 100 JSON files\n",
      "Completed nfl/Dallas Cowboys offense: 0 posts in 0.0s\n",
      "Skipping nfl/Dallas Cowboys defense - contains 100 JSON files\n",
      "Completed nfl/Dallas Cowboys defense: 0 posts in 0.0s\n",
      "Skipping nfl/New York Giants win - contains 100 JSON files\n",
      "Completed nfl/New York Giants win: 0 posts in 0.0s\n",
      "Skipping nfl/New York Giants loss - contains 100 JSON files\n",
      "Completed nfl/New York Giants loss: 0 posts in 0.0s\n",
      "Skipping nfl/New York Giants defeated - contains 100 JSON files\n",
      "Completed nfl/New York Giants defeated: 0 posts in 0.0s\n",
      "Skipping nfl/New York Giants upset - contains 100 JSON files\n",
      "Completed nfl/New York Giants upset: 0 posts in 0.0s\n",
      "Skipping nfl/New York Giants offense - contains 100 JSON files\n",
      "Completed nfl/New York Giants offense: 0 posts in 0.0s\n",
      "Skipping nfl/New York Giants defense - contains 100 JSON files\n",
      "Completed nfl/New York Giants defense: 0 posts in 0.0s\n",
      "Skipping nfl/Philadelphia Eagles win - contains 100 JSON files\n",
      "Completed nfl/Philadelphia Eagles win: 0 posts in 0.0s\n",
      "Skipping nfl/Philadelphia Eagles loss - contains 100 JSON files\n",
      "Completed nfl/Philadelphia Eagles loss: 0 posts in 0.0s\n",
      "Skipping nfl/Philadelphia Eagles defeated - contains 100 JSON files\n",
      "Completed nfl/Philadelphia Eagles defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Philadelphia Eagles upset - contains 100 JSON files\n",
      "Completed nfl/Philadelphia Eagles upset: 0 posts in 0.0s\n",
      "Skipping nfl/Philadelphia Eagles offense - contains 100 JSON files\n",
      "Completed nfl/Philadelphia Eagles offense: 0 posts in 0.0s\n",
      "Skipping nfl/Philadelphia Eagles defense - contains 100 JSON files\n",
      "Completed nfl/Philadelphia Eagles defense: 0 posts in 0.0s\n",
      "Skipping nfl/Washington Commanders win - contains 100 JSON files\n",
      "Completed nfl/Washington Commanders win: 0 posts in 0.0s\n",
      "Skipping nfl/Washington Commanders loss - contains 100 JSON files\n",
      "Completed nfl/Washington Commanders loss: 0 posts in 0.0s\n",
      "Skipping nfl/Washington Commanders defeated - contains 100 JSON files\n",
      "Completed nfl/Washington Commanders defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Washington Commanders upset - contains 100 JSON files\n",
      "Completed nfl/Washington Commanders upset: 0 posts in 0.0s\n",
      "Skipping nfl/Washington Commanders offense - contains 100 JSON files\n",
      "Completed nfl/Washington Commanders offense: 0 posts in 0.0s\n",
      "Skipping nfl/Washington Commanders defense - contains 100 JSON files\n",
      "Completed nfl/Washington Commanders defense: 0 posts in 0.0s\n",
      "Skipping nfl/Arizona Cardinals win - contains 100 JSON files\n",
      "Completed nfl/Arizona Cardinals win: 0 posts in 0.0s\n",
      "Skipping nfl/Arizona Cardinals loss - contains 100 JSON files\n",
      "Completed nfl/Arizona Cardinals loss: 0 posts in 0.0s\n",
      "Skipping nfl/Arizona Cardinals defeated - contains 100 JSON files\n",
      "Completed nfl/Arizona Cardinals defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Arizona Cardinals upset - contains 100 JSON files\n",
      "Completed nfl/Arizona Cardinals upset: 0 posts in 0.0s\n",
      "Skipping nfl/Arizona Cardinals offense - contains 100 JSON files\n",
      "Completed nfl/Arizona Cardinals offense: 0 posts in 0.0s\n",
      "Skipping nfl/Arizona Cardinals defense - contains 100 JSON files\n",
      "Completed nfl/Arizona Cardinals defense: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Rams win - contains 100 JSON files\n",
      "Completed nfl/Los Angeles Rams win: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Rams loss - contains 100 JSON files\n",
      "Completed nfl/Los Angeles Rams loss: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Rams defeated - contains 100 JSON files\n",
      "Completed nfl/Los Angeles Rams defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Rams upset - contains 100 JSON files\n",
      "Completed nfl/Los Angeles Rams upset: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Rams offense - contains 100 JSON files\n",
      "Completed nfl/Los Angeles Rams offense: 0 posts in 0.0s\n",
      "Skipping nfl/Los Angeles Rams defense - contains 100 JSON files\n",
      "Completed nfl/Los Angeles Rams defense: 0 posts in 0.0s\n",
      "Skipping nfl/San Francisco 49ers win - contains 100 JSON files\n",
      "Completed nfl/San Francisco 49ers win: 0 posts in 0.0s\n",
      "Skipping nfl/San Francisco 49ers loss - contains 100 JSON files\n",
      "Completed nfl/San Francisco 49ers loss: 0 posts in 0.0s\n",
      "Skipping nfl/San Francisco 49ers defeated - contains 100 JSON files\n",
      "Completed nfl/San Francisco 49ers defeated: 0 posts in 0.0s\n",
      "Skipping nfl/San Francisco 49ers upset - contains 100 JSON files\n",
      "Completed nfl/San Francisco 49ers upset: 0 posts in 0.0s\n",
      "Skipping nfl/San Francisco 49ers offense - contains 100 JSON files\n",
      "Completed nfl/San Francisco 49ers offense: 0 posts in 0.0s\n",
      "Skipping nfl/San Francisco 49ers defense - contains 100 JSON files\n",
      "Completed nfl/San Francisco 49ers defense: 0 posts in 0.0s\n",
      "Skipping nfl/Seattle Seahawks win - contains 100 JSON files\n",
      "Completed nfl/Seattle Seahawks win: 0 posts in 0.0s\n",
      "Skipping nfl/Seattle Seahawks loss - contains 100 JSON files\n",
      "Completed nfl/Seattle Seahawks loss: 0 posts in 0.0s\n",
      "Skipping nfl/Seattle Seahawks defeated - contains 100 JSON files\n",
      "Completed nfl/Seattle Seahawks defeated: 0 posts in 0.0s\n",
      "Skipping nfl/Seattle Seahawks upset - contains 100 JSON files\n",
      "Completed nfl/Seattle Seahawks upset: 0 posts in 0.0s\n",
      "Skipping nfl/Seattle Seahawks offense - contains 100 JSON files\n",
      "Completed nfl/Seattle Seahawks offense: 0 posts in 0.0s\n",
      "Skipping nfl/Seattle Seahawks defense - contains 100 JSON files\n",
      "Completed nfl/Seattle Seahawks defense: 0 posts in 0.0s\n",
      "Skipping nfl/Patrick Mahomes performance - contains 100 JSON files\n",
      "Completed nfl/Patrick Mahomes performance: 0 posts in 0.0s\n",
      "Skipping nfl/Patrick Mahomes MVP - contains 100 JSON files\n",
      "Completed nfl/Patrick Mahomes MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Patrick Mahomes breakout - contains 100 JSON files\n",
      "Completed nfl/Patrick Mahomes breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Patrick Mahomes struggling - contains 100 JSON files\n",
      "Completed nfl/Patrick Mahomes struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Patrick Mahomes strength - contains 100 JSON files\n",
      "Completed nfl/Patrick Mahomes strength: 0 posts in 0.0s\n",
      "Skipping nfl/Patrick Mahomes weakness - contains 100 JSON files\n",
      "Completed nfl/Patrick Mahomes weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Patrick Mahomes passing - contains 100 JSON files\n",
      "Completed nfl/Patrick Mahomes passing: 0 posts in 0.0s\n",
      "Skipping nfl/Patrick Mahomes rushing - contains 100 JSON files\n",
      "Completed nfl/Patrick Mahomes rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Jefferson performance - contains 100 JSON files\n",
      "Completed nfl/Justin Jefferson performance: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Jefferson MVP - contains 100 JSON files\n",
      "Completed nfl/Justin Jefferson MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Jefferson breakout - contains 100 JSON files\n",
      "Completed nfl/Justin Jefferson breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Jefferson struggling - contains 100 JSON files\n",
      "Completed nfl/Justin Jefferson struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Jefferson strength - contains 100 JSON files\n",
      "Completed nfl/Justin Jefferson strength: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Jefferson weakness - contains 100 JSON files\n",
      "Completed nfl/Justin Jefferson weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Jefferson passing - contains 100 JSON files\n",
      "Completed nfl/Justin Jefferson passing: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Jefferson rushing - contains 100 JSON files\n",
      "Completed nfl/Justin Jefferson rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Nick Bosa performance - contains 100 JSON files\n",
      "Completed nfl/Nick Bosa performance: 0 posts in 0.0s\n",
      "Skipping nfl/Nick Bosa MVP - contains 100 JSON files\n",
      "Completed nfl/Nick Bosa MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Nick Bosa breakout - contains 100 JSON files\n",
      "Completed nfl/Nick Bosa breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Nick Bosa struggling - contains 100 JSON files\n",
      "Completed nfl/Nick Bosa struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Nick Bosa strength - contains 100 JSON files\n",
      "Completed nfl/Nick Bosa strength: 0 posts in 0.0s\n",
      "Skipping nfl/Nick Bosa weakness - contains 100 JSON files\n",
      "Completed nfl/Nick Bosa weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Nick Bosa passing - contains 100 JSON files\n",
      "Completed nfl/Nick Bosa passing: 0 posts in 0.0s\n",
      "Skipping nfl/Nick Bosa rushing - contains 100 JSON files\n",
      "Completed nfl/Nick Bosa rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Travis Kelce performance - contains 100 JSON files\n",
      "Completed nfl/Travis Kelce performance: 0 posts in 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping nfl/Travis Kelce MVP - contains 100 JSON files\n",
      "Completed nfl/Travis Kelce MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Travis Kelce breakout - contains 100 JSON files\n",
      "Completed nfl/Travis Kelce breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Travis Kelce struggling - contains 100 JSON files\n",
      "Completed nfl/Travis Kelce struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Travis Kelce strength - contains 100 JSON files\n",
      "Completed nfl/Travis Kelce strength: 0 posts in 0.0s\n",
      "Skipping nfl/Travis Kelce weakness - contains 100 JSON files\n",
      "Completed nfl/Travis Kelce weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Travis Kelce passing - contains 100 JSON files\n",
      "Completed nfl/Travis Kelce passing: 0 posts in 0.0s\n",
      "Skipping nfl/Travis Kelce rushing - contains 100 JSON files\n",
      "Completed nfl/Travis Kelce rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Tua Tagovailoa performance - contains 100 JSON files\n",
      "Completed nfl/Tua Tagovailoa performance: 0 posts in 0.0s\n",
      "Skipping nfl/Tua Tagovailoa MVP - contains 100 JSON files\n",
      "Completed nfl/Tua Tagovailoa MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Tua Tagovailoa breakout - contains 100 JSON files\n",
      "Completed nfl/Tua Tagovailoa breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Tua Tagovailoa struggling - contains 100 JSON files\n",
      "Completed nfl/Tua Tagovailoa struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Tua Tagovailoa strength - contains 100 JSON files\n",
      "Completed nfl/Tua Tagovailoa strength: 0 posts in 0.0s\n",
      "Skipping nfl/Tua Tagovailoa weakness - contains 100 JSON files\n",
      "Completed nfl/Tua Tagovailoa weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Tua Tagovailoa passing - contains 100 JSON files\n",
      "Completed nfl/Tua Tagovailoa passing: 0 posts in 0.0s\n",
      "Skipping nfl/Tua Tagovailoa rushing - contains 100 JSON files\n",
      "Completed nfl/Tua Tagovailoa rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Micah Parsons performance - contains 100 JSON files\n",
      "Completed nfl/Micah Parsons performance: 0 posts in 0.0s\n",
      "Skipping nfl/Micah Parsons MVP - contains 100 JSON files\n",
      "Completed nfl/Micah Parsons MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Micah Parsons breakout - contains 100 JSON files\n",
      "Completed nfl/Micah Parsons breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Micah Parsons struggling - contains 100 JSON files\n",
      "Completed nfl/Micah Parsons struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Micah Parsons strength - contains 100 JSON files\n",
      "Completed nfl/Micah Parsons strength: 0 posts in 0.0s\n",
      "Skipping nfl/Micah Parsons weakness - contains 100 JSON files\n",
      "Completed nfl/Micah Parsons weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Micah Parsons passing - contains 100 JSON files\n",
      "Completed nfl/Micah Parsons passing: 0 posts in 0.0s\n",
      "Skipping nfl/Micah Parsons rushing - contains 100 JSON files\n",
      "Completed nfl/Micah Parsons rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Tyreek Hill performance - contains 100 JSON files\n",
      "Completed nfl/Tyreek Hill performance: 0 posts in 0.0s\n",
      "Skipping nfl/Tyreek Hill MVP - contains 100 JSON files\n",
      "Completed nfl/Tyreek Hill MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Tyreek Hill breakout - contains 100 JSON files\n",
      "Completed nfl/Tyreek Hill breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Tyreek Hill struggling - contains 100 JSON files\n",
      "Completed nfl/Tyreek Hill struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Tyreek Hill strength - contains 100 JSON files\n",
      "Completed nfl/Tyreek Hill strength: 0 posts in 0.0s\n",
      "Skipping nfl/Tyreek Hill weakness - contains 100 JSON files\n",
      "Completed nfl/Tyreek Hill weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Tyreek Hill passing - contains 100 JSON files\n",
      "Completed nfl/Tyreek Hill passing: 0 posts in 0.0s\n",
      "Skipping nfl/Tyreek Hill rushing - contains 100 JSON files\n",
      "Completed nfl/Tyreek Hill rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Joe Burrow performance - contains 100 JSON files\n",
      "Completed nfl/Joe Burrow performance: 0 posts in 0.0s\n",
      "Skipping nfl/Joe Burrow MVP - contains 100 JSON files\n",
      "Completed nfl/Joe Burrow MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Joe Burrow breakout - contains 100 JSON files\n",
      "Completed nfl/Joe Burrow breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Joe Burrow struggling - contains 100 JSON files\n",
      "Completed nfl/Joe Burrow struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Joe Burrow strength - contains 100 JSON files\n",
      "Completed nfl/Joe Burrow strength: 0 posts in 0.0s\n",
      "Skipping nfl/Joe Burrow weakness - contains 100 JSON files\n",
      "Completed nfl/Joe Burrow weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Joe Burrow passing - contains 100 JSON files\n",
      "Completed nfl/Joe Burrow passing: 0 posts in 0.0s\n",
      "Skipping nfl/Joe Burrow rushing - contains 100 JSON files\n",
      "Completed nfl/Joe Burrow rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Christian McCaffrey performance - contains 100 JSON files\n",
      "Completed nfl/Christian McCaffrey performance: 0 posts in 0.0s\n",
      "Skipping nfl/Christian McCaffrey MVP - contains 100 JSON files\n",
      "Completed nfl/Christian McCaffrey MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Christian McCaffrey breakout - contains 100 JSON files\n",
      "Completed nfl/Christian McCaffrey breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Christian McCaffrey struggling - contains 100 JSON files\n",
      "Completed nfl/Christian McCaffrey struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Christian McCaffrey strength - contains 100 JSON files\n",
      "Completed nfl/Christian McCaffrey strength: 0 posts in 0.0s\n",
      "Skipping nfl/Christian McCaffrey weakness - contains 100 JSON files\n",
      "Completed nfl/Christian McCaffrey weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Christian McCaffrey passing - contains 100 JSON files\n",
      "Completed nfl/Christian McCaffrey passing: 0 posts in 0.0s\n",
      "Skipping nfl/Christian McCaffrey rushing - contains 100 JSON files\n",
      "Completed nfl/Christian McCaffrey rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Aaron Donald performance - contains 100 JSON files\n",
      "Completed nfl/Aaron Donald performance: 0 posts in 0.0s\n",
      "Skipping nfl/Aaron Donald MVP - contains 100 JSON files\n",
      "Completed nfl/Aaron Donald MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Aaron Donald breakout - contains 100 JSON files\n",
      "Completed nfl/Aaron Donald breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Aaron Donald struggling - contains 100 JSON files\n",
      "Completed nfl/Aaron Donald struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Aaron Donald strength - contains 100 JSON files\n",
      "Completed nfl/Aaron Donald strength: 0 posts in 0.0s\n",
      "Skipping nfl/Aaron Donald weakness - contains 100 JSON files\n",
      "Completed nfl/Aaron Donald weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Aaron Donald passing - contains 100 JSON files\n",
      "Completed nfl/Aaron Donald passing: 0 posts in 0.0s\n",
      "Skipping nfl/Aaron Donald rushing - contains 100 JSON files\n",
      "Completed nfl/Aaron Donald rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Josh Allen performance - contains 100 JSON files\n",
      "Completed nfl/Josh Allen performance: 0 posts in 0.0s\n",
      "Skipping nfl/Josh Allen MVP - contains 100 JSON files\n",
      "Completed nfl/Josh Allen MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Josh Allen breakout - contains 100 JSON files\n",
      "Completed nfl/Josh Allen breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Josh Allen struggling - contains 100 JSON files\n",
      "Completed nfl/Josh Allen struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Josh Allen strength - contains 100 JSON files\n",
      "Completed nfl/Josh Allen strength: 0 posts in 0.0s\n",
      "Skipping nfl/Josh Allen weakness - contains 100 JSON files\n",
      "Completed nfl/Josh Allen weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Josh Allen passing - contains 100 JSON files\n",
      "Completed nfl/Josh Allen passing: 0 posts in 0.0s\n",
      "Skipping nfl/Josh Allen rushing - contains 100 JSON files\n",
      "Completed nfl/Josh Allen rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Lamar Jackson performance - contains 100 JSON files\n",
      "Completed nfl/Lamar Jackson performance: 0 posts in 0.0s\n",
      "Skipping nfl/Lamar Jackson MVP - contains 100 JSON files\n",
      "Completed nfl/Lamar Jackson MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Lamar Jackson breakout - contains 100 JSON files\n",
      "Completed nfl/Lamar Jackson breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Lamar Jackson struggling - contains 100 JSON files\n",
      "Completed nfl/Lamar Jackson struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Lamar Jackson strength - contains 100 JSON files\n",
      "Completed nfl/Lamar Jackson strength: 0 posts in 0.0s\n",
      "Skipping nfl/Lamar Jackson weakness - contains 100 JSON files\n",
      "Completed nfl/Lamar Jackson weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Lamar Jackson passing - contains 100 JSON files\n",
      "Completed nfl/Lamar Jackson passing: 0 posts in 0.0s\n",
      "Skipping nfl/Lamar Jackson rushing - contains 100 JSON files\n",
      "Completed nfl/Lamar Jackson rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Ja'Marr Chase performance - contains 100 JSON files\n",
      "Completed nfl/Ja'Marr Chase performance: 0 posts in 0.0s\n",
      "Skipping nfl/Ja'Marr Chase MVP - contains 100 JSON files\n",
      "Completed nfl/Ja'Marr Chase MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Ja'Marr Chase breakout - contains 100 JSON files\n",
      "Completed nfl/Ja'Marr Chase breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Ja'Marr Chase struggling - contains 100 JSON files\n",
      "Completed nfl/Ja'Marr Chase struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Ja'Marr Chase strength - contains 100 JSON files\n",
      "Completed nfl/Ja'Marr Chase strength: 0 posts in 0.0s\n",
      "Skipping nfl/Ja'Marr Chase weakness - contains 100 JSON files\n",
      "Completed nfl/Ja'Marr Chase weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Ja'Marr Chase passing - contains 100 JSON files\n",
      "Completed nfl/Ja'Marr Chase passing: 0 posts in 0.0s\n",
      "Skipping nfl/Ja'Marr Chase rushing - contains 100 JSON files\n",
      "Completed nfl/Ja'Marr Chase rushing: 0 posts in 0.0s\n",
      "Skipping nfl/T.J. Watt performance - contains 100 JSON files\n",
      "Completed nfl/T.J. Watt performance: 0 posts in 0.0s\n",
      "Skipping nfl/T.J. Watt MVP - contains 100 JSON files\n",
      "Completed nfl/T.J. Watt MVP: 0 posts in 0.0s\n",
      "Skipping nfl/T.J. Watt breakout - contains 100 JSON files\n",
      "Completed nfl/T.J. Watt breakout: 0 posts in 0.0s\n",
      "Skipping nfl/T.J. Watt struggling - contains 100 JSON files\n",
      "Completed nfl/T.J. Watt struggling: 0 posts in 0.0s\n",
      "Skipping nfl/T.J. Watt strength - contains 100 JSON files\n",
      "Completed nfl/T.J. Watt strength: 0 posts in 0.0s\n",
      "Skipping nfl/T.J. Watt weakness - contains 100 JSON files\n",
      "Completed nfl/T.J. Watt weakness: 0 posts in 0.0s\n",
      "Skipping nfl/T.J. Watt passing - contains 100 JSON files\n",
      "Completed nfl/T.J. Watt passing: 0 posts in 0.0s\n",
      "Skipping nfl/T.J. Watt rushing - contains 100 JSON files\n",
      "Completed nfl/T.J. Watt rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Cooper Kupp performance - contains 100 JSON files\n",
      "Completed nfl/Cooper Kupp performance: 0 posts in 0.0s\n",
      "Skipping nfl/Cooper Kupp MVP - contains 100 JSON files\n",
      "Completed nfl/Cooper Kupp MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Cooper Kupp breakout - contains 100 JSON files\n",
      "Completed nfl/Cooper Kupp breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Cooper Kupp struggling - contains 100 JSON files\n",
      "Completed nfl/Cooper Kupp struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Cooper Kupp strength - contains 100 JSON files\n",
      "Completed nfl/Cooper Kupp strength: 0 posts in 0.0s\n",
      "Skipping nfl/Cooper Kupp weakness - contains 100 JSON files\n",
      "Completed nfl/Cooper Kupp weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Cooper Kupp passing - contains 100 JSON files\n",
      "Completed nfl/Cooper Kupp passing: 0 posts in 0.0s\n",
      "Skipping nfl/Cooper Kupp rushing - contains 100 JSON files\n",
      "Completed nfl/Cooper Kupp rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Dak Prescott performance - contains 100 JSON files\n",
      "Completed nfl/Dak Prescott performance: 0 posts in 0.0s\n",
      "Skipping nfl/Dak Prescott MVP - contains 100 JSON files\n",
      "Completed nfl/Dak Prescott MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Dak Prescott breakout - contains 100 JSON files\n",
      "Completed nfl/Dak Prescott breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Dak Prescott struggling - contains 100 JSON files\n",
      "Completed nfl/Dak Prescott struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Dak Prescott strength - contains 100 JSON files\n",
      "Completed nfl/Dak Prescott strength: 0 posts in 0.0s\n",
      "Skipping nfl/Dak Prescott weakness - contains 100 JSON files\n",
      "Completed nfl/Dak Prescott weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Dak Prescott passing - contains 100 JSON files\n",
      "Completed nfl/Dak Prescott passing: 0 posts in 0.0s\n",
      "Skipping nfl/Dak Prescott rushing - contains 100 JSON files\n",
      "Completed nfl/Dak Prescott rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Derick Henry performance - contains 100 JSON files\n",
      "Completed nfl/Derick Henry performance: 0 posts in 0.0s\n",
      "Skipping nfl/Derick Henry MVP - contains 100 JSON files\n",
      "Completed nfl/Derick Henry MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Derick Henry breakout - contains 100 JSON files\n",
      "Completed nfl/Derick Henry breakout: 0 posts in 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping nfl/Derick Henry struggling - contains 100 JSON files\n",
      "Completed nfl/Derick Henry struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Derick Henry strength - contains 100 JSON files\n",
      "Completed nfl/Derick Henry strength: 0 posts in 0.0s\n",
      "Skipping nfl/Derick Henry weakness - contains 100 JSON files\n",
      "Completed nfl/Derick Henry weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Derick Henry passing - contains 100 JSON files\n",
      "Completed nfl/Derick Henry passing: 0 posts in 0.0s\n",
      "Skipping nfl/Derick Henry rushing - contains 100 JSON files\n",
      "Completed nfl/Derick Henry rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Sauce Gardner performance - contains 100 JSON files\n",
      "Completed nfl/Sauce Gardner performance: 0 posts in 0.0s\n",
      "Skipping nfl/Sauce Gardner MVP - contains 100 JSON files\n",
      "Completed nfl/Sauce Gardner MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Sauce Gardner breakout - contains 100 JSON files\n",
      "Completed nfl/Sauce Gardner breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Sauce Gardner struggling - contains 100 JSON files\n",
      "Completed nfl/Sauce Gardner struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Sauce Gardner strength - contains 100 JSON files\n",
      "Completed nfl/Sauce Gardner strength: 0 posts in 0.0s\n",
      "Skipping nfl/Sauce Gardner weakness - contains 100 JSON files\n",
      "Completed nfl/Sauce Gardner weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Sauce Gardner passing - contains 100 JSON files\n",
      "Completed nfl/Sauce Gardner passing: 0 posts in 0.0s\n",
      "Skipping nfl/Sauce Gardner rushing - contains 100 JSON files\n",
      "Completed nfl/Sauce Gardner rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Jalen Hurts performance - contains 100 JSON files\n",
      "Completed nfl/Jalen Hurts performance: 0 posts in 0.0s\n",
      "Skipping nfl/Jalen Hurts MVP - contains 100 JSON files\n",
      "Completed nfl/Jalen Hurts MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Jalen Hurts breakout - contains 100 JSON files\n",
      "Completed nfl/Jalen Hurts breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Jalen Hurts struggling - contains 100 JSON files\n",
      "Completed nfl/Jalen Hurts struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Jalen Hurts strength - contains 100 JSON files\n",
      "Completed nfl/Jalen Hurts strength: 0 posts in 0.0s\n",
      "Skipping nfl/Jalen Hurts weakness - contains 100 JSON files\n",
      "Completed nfl/Jalen Hurts weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Jalen Hurts passing - contains 100 JSON files\n",
      "Completed nfl/Jalen Hurts passing: 0 posts in 0.0s\n",
      "Skipping nfl/Jalen Hurts rushing - contains 100 JSON files\n",
      "Completed nfl/Jalen Hurts rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Myles Garrett performance - contains 100 JSON files\n",
      "Completed nfl/Myles Garrett performance: 0 posts in 0.0s\n",
      "Skipping nfl/Myles Garrett MVP - contains 100 JSON files\n",
      "Completed nfl/Myles Garrett MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Myles Garrett breakout - contains 100 JSON files\n",
      "Completed nfl/Myles Garrett breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Myles Garrett struggling - contains 100 JSON files\n",
      "Completed nfl/Myles Garrett struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Myles Garrett strength - contains 100 JSON files\n",
      "Completed nfl/Myles Garrett strength: 0 posts in 0.0s\n",
      "Skipping nfl/Myles Garrett weakness - contains 100 JSON files\n",
      "Completed nfl/Myles Garrett weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Myles Garrett passing - contains 100 JSON files\n",
      "Completed nfl/Myles Garrett passing: 0 posts in 0.0s\n",
      "Skipping nfl/Myles Garrett rushing - contains 100 JSON files\n",
      "Completed nfl/Myles Garrett rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Trevor Lawrence performance - contains 100 JSON files\n",
      "Completed nfl/Trevor Lawrence performance: 0 posts in 0.0s\n",
      "Skipping nfl/Trevor Lawrence MVP - contains 100 JSON files\n",
      "Completed nfl/Trevor Lawrence MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Trevor Lawrence breakout - contains 100 JSON files\n",
      "Completed nfl/Trevor Lawrence breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Trevor Lawrence struggling - contains 100 JSON files\n",
      "Completed nfl/Trevor Lawrence struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Trevor Lawrence strength - contains 100 JSON files\n",
      "Completed nfl/Trevor Lawrence strength: 0 posts in 0.0s\n",
      "Skipping nfl/Trevor Lawrence weakness - contains 100 JSON files\n",
      "Completed nfl/Trevor Lawrence weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Trevor Lawrence passing - contains 100 JSON files\n",
      "Completed nfl/Trevor Lawrence passing: 0 posts in 0.0s\n",
      "Skipping nfl/Trevor Lawrence rushing - contains 100 JSON files\n",
      "Completed nfl/Trevor Lawrence rushing: 0 posts in 0.0s\n",
      "Skipping nfl/A.J. Brown performance - contains 100 JSON files\n",
      "Completed nfl/A.J. Brown performance: 0 posts in 0.0s\n",
      "Skipping nfl/A.J. Brown MVP - contains 100 JSON files\n",
      "Completed nfl/A.J. Brown MVP: 0 posts in 0.0s\n",
      "Skipping nfl/A.J. Brown breakout - contains 100 JSON files\n",
      "Completed nfl/A.J. Brown breakout: 0 posts in 0.0s\n",
      "Skipping nfl/A.J. Brown struggling - contains 100 JSON files\n",
      "Completed nfl/A.J. Brown struggling: 0 posts in 0.0s\n",
      "Skipping nfl/A.J. Brown strength - contains 100 JSON files\n",
      "Completed nfl/A.J. Brown strength: 0 posts in 0.0s\n",
      "Skipping nfl/A.J. Brown weakness - contains 100 JSON files\n",
      "Completed nfl/A.J. Brown weakness: 0 posts in 0.0s\n",
      "Skipping nfl/A.J. Brown passing - contains 100 JSON files\n",
      "Completed nfl/A.J. Brown passing: 0 posts in 0.0s\n",
      "Skipping nfl/A.J. Brown rushing - contains 100 JSON files\n",
      "Completed nfl/A.J. Brown rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Fred Warner performance - contains 100 JSON files\n",
      "Completed nfl/Fred Warner performance: 0 posts in 0.0s\n",
      "Skipping nfl/Fred Warner MVP - contains 100 JSON files\n",
      "Completed nfl/Fred Warner MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Fred Warner breakout - contains 100 JSON files\n",
      "Completed nfl/Fred Warner breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Fred Warner struggling - contains 100 JSON files\n",
      "Completed nfl/Fred Warner struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Fred Warner strength - contains 100 JSON files\n",
      "Completed nfl/Fred Warner strength: 0 posts in 0.0s\n",
      "Skipping nfl/Fred Warner weakness - contains 100 JSON files\n",
      "Completed nfl/Fred Warner weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Fred Warner passing - contains 100 JSON files\n",
      "Completed nfl/Fred Warner passing: 0 posts in 0.0s\n",
      "Skipping nfl/Fred Warner rushing - contains 100 JSON files\n",
      "Completed nfl/Fred Warner rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Herbert performance - contains 100 JSON files\n",
      "Completed nfl/Justin Herbert performance: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Herbert MVP - contains 100 JSON files\n",
      "Completed nfl/Justin Herbert MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Herbert breakout - contains 100 JSON files\n",
      "Completed nfl/Justin Herbert breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Herbert struggling - contains 100 JSON files\n",
      "Completed nfl/Justin Herbert struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Herbert strength - contains 100 JSON files\n",
      "Completed nfl/Justin Herbert strength: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Herbert weakness - contains 100 JSON files\n",
      "Completed nfl/Justin Herbert weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Herbert passing - contains 100 JSON files\n",
      "Completed nfl/Justin Herbert passing: 0 posts in 0.0s\n",
      "Skipping nfl/Justin Herbert rushing - contains 100 JSON files\n",
      "Completed nfl/Justin Herbert rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Bijan Robinson performance - contains 100 JSON files\n",
      "Completed nfl/Bijan Robinson performance: 0 posts in 0.0s\n",
      "Skipping nfl/Bijan Robinson MVP - contains 100 JSON files\n",
      "Completed nfl/Bijan Robinson MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Bijan Robinson breakout - contains 100 JSON files\n",
      "Completed nfl/Bijan Robinson breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Bijan Robinson struggling - contains 100 JSON files\n",
      "Completed nfl/Bijan Robinson struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Bijan Robinson strength - contains 100 JSON files\n",
      "Completed nfl/Bijan Robinson strength: 0 posts in 0.0s\n",
      "Skipping nfl/Bijan Robinson weakness - contains 100 JSON files\n",
      "Completed nfl/Bijan Robinson weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Bijan Robinson passing - contains 100 JSON files\n",
      "Completed nfl/Bijan Robinson passing: 0 posts in 0.0s\n",
      "Skipping nfl/Bijan Robinson rushing - contains 100 JSON files\n",
      "Completed nfl/Bijan Robinson rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Dexter Lawrence performance - contains 100 JSON files\n",
      "Completed nfl/Dexter Lawrence performance: 0 posts in 0.0s\n",
      "Skipping nfl/Dexter Lawrence MVP - contains 100 JSON files\n",
      "Completed nfl/Dexter Lawrence MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Dexter Lawrence breakout - contains 100 JSON files\n",
      "Completed nfl/Dexter Lawrence breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Dexter Lawrence struggling - contains 100 JSON files\n",
      "Completed nfl/Dexter Lawrence struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Dexter Lawrence strength - contains 100 JSON files\n",
      "Completed nfl/Dexter Lawrence strength: 0 posts in 0.0s\n",
      "Skipping nfl/Dexter Lawrence weakness - contains 100 JSON files\n",
      "Completed nfl/Dexter Lawrence weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Dexter Lawrence passing - contains 100 JSON files\n",
      "Completed nfl/Dexter Lawrence passing: 0 posts in 0.0s\n",
      "Skipping nfl/Dexter Lawrence rushing - contains 100 JSON files\n",
      "Completed nfl/Dexter Lawrence rushing: 0 posts in 0.0s\n",
      "Skipping nfl/CeeDee Lamb performance - contains 100 JSON files\n",
      "Completed nfl/CeeDee Lamb performance: 0 posts in 0.0s\n",
      "Skipping nfl/CeeDee Lamb MVP - contains 100 JSON files\n",
      "Completed nfl/CeeDee Lamb MVP: 0 posts in 0.0s\n",
      "Skipping nfl/CeeDee Lamb breakout - contains 100 JSON files\n",
      "Completed nfl/CeeDee Lamb breakout: 0 posts in 0.0s\n",
      "Skipping nfl/CeeDee Lamb struggling - contains 100 JSON files\n",
      "Completed nfl/CeeDee Lamb struggling: 0 posts in 0.0s\n",
      "Skipping nfl/CeeDee Lamb strength - contains 100 JSON files\n",
      "Completed nfl/CeeDee Lamb strength: 0 posts in 0.0s\n",
      "Skipping nfl/CeeDee Lamb weakness - contains 100 JSON files\n",
      "Completed nfl/CeeDee Lamb weakness: 0 posts in 0.0s\n",
      "Skipping nfl/CeeDee Lamb passing - contains 100 JSON files\n",
      "Completed nfl/CeeDee Lamb passing: 0 posts in 0.0s\n",
      "Skipping nfl/CeeDee Lamb rushing - contains 100 JSON files\n",
      "Completed nfl/CeeDee Lamb rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Trent Williams performance - contains 100 JSON files\n",
      "Completed nfl/Trent Williams performance: 0 posts in 0.0s\n",
      "Skipping nfl/Trent Williams MVP - contains 100 JSON files\n",
      "Completed nfl/Trent Williams MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Trent Williams breakout - contains 100 JSON files\n",
      "Completed nfl/Trent Williams breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Trent Williams struggling - contains 100 JSON files\n",
      "Completed nfl/Trent Williams struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Trent Williams strength - contains 100 JSON files\n",
      "Completed nfl/Trent Williams strength: 0 posts in 0.0s\n",
      "Skipping nfl/Trent Williams weakness - contains 100 JSON files\n",
      "Completed nfl/Trent Williams weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Trent Williams passing - contains 100 JSON files\n",
      "Completed nfl/Trent Williams passing: 0 posts in 0.0s\n",
      "Skipping nfl/Trent Williams rushing - contains 100 JSON files\n",
      "Completed nfl/Trent Williams rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Minkah Fitzpatrick performance - contains 100 JSON files\n",
      "Completed nfl/Minkah Fitzpatrick performance: 0 posts in 0.0s\n",
      "Skipping nfl/Minkah Fitzpatrick MVP - contains 100 JSON files\n",
      "Completed nfl/Minkah Fitzpatrick MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Minkah Fitzpatrick breakout - contains 100 JSON files\n",
      "Completed nfl/Minkah Fitzpatrick breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Minkah Fitzpatrick struggling - contains 100 JSON files\n",
      "Completed nfl/Minkah Fitzpatrick struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Minkah Fitzpatrick strength - contains 100 JSON files\n",
      "Completed nfl/Minkah Fitzpatrick strength: 0 posts in 0.0s\n",
      "Skipping nfl/Minkah Fitzpatrick weakness - contains 100 JSON files\n",
      "Completed nfl/Minkah Fitzpatrick weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Minkah Fitzpatrick passing - contains 100 JSON files\n",
      "Completed nfl/Minkah Fitzpatrick passing: 0 posts in 0.0s\n",
      "Skipping nfl/Minkah Fitzpatrick rushing - contains 100 JSON files\n",
      "Completed nfl/Minkah Fitzpatrick rushing: 0 posts in 0.0s\n",
      "Skipping nfl/Ammon-Ra St. Brown performance - contains 100 JSON files\n",
      "Completed nfl/Ammon-Ra St. Brown performance: 0 posts in 0.0s\n",
      "Skipping nfl/Ammon-Ra St. Brown MVP - contains 100 JSON files\n",
      "Completed nfl/Ammon-Ra St. Brown MVP: 0 posts in 0.0s\n",
      "Skipping nfl/Ammon-Ra St. Brown breakout - contains 100 JSON files\n",
      "Completed nfl/Ammon-Ra St. Brown breakout: 0 posts in 0.0s\n",
      "Skipping nfl/Ammon-Ra St. Brown struggling - contains 100 JSON files\n",
      "Completed nfl/Ammon-Ra St. Brown struggling: 0 posts in 0.0s\n",
      "Skipping nfl/Ammon-Ra St. Brown strength - contains 100 JSON files\n",
      "Completed nfl/Ammon-Ra St. Brown strength: 0 posts in 0.0s\n",
      "Skipping nfl/Ammon-Ra St. Brown weakness - contains 100 JSON files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress: 100%|██████████| 432/432 [00:00<00:00, 534.26it/s, Current: nfl/Ammon-Ra St. Brown r]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed nfl/Ammon-Ra St. Brown weakness: 0 posts in 0.0s\n",
      "Skipping nfl/Ammon-Ra St. Brown passing - contains 100 JSON files\n",
      "Completed nfl/Ammon-Ra St. Brown passing: 0 posts in 0.0s\n",
      "Skipping nfl/Ammon-Ra St. Brown rushing - contains 100 JSON files\n",
      "Completed nfl/Ammon-Ra St. Brown rushing: 0 posts in 0.0s\n",
      "\n",
      "Finished. Total posts processed: 0 across 432 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import praw\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"biWPqknyVXfpa4gvym_peA\",\n",
    "    client_secret=\"N5e9zLgIiOqcUqYgChbVfrI8XUIqmA\",\n",
    "    user_agent=\"Common_Salt2614\"\n",
    ")\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Sanitize the filename by replacing problematic characters.\"\"\"\n",
    "    return ''.join(c if c.isalnum() or c in (' ', '_') else '_' for c in filename).strip().replace(' ', '_')\n",
    "\n",
    "def process_subreddit_query(subreddit_name, search_query):\n",
    "    \"\"\"\n",
    "    Process a single subreddit for a given search query.\n",
    "    Shows progress bar for submissions and handles rate limits.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing subreddit {subreddit_name}: {e}\")\n",
    "        return 0\n",
    "\n",
    "    # Create folder structure\n",
    "    subreddit_folder = os.path.join(output_folder_base, sanitize_filename(subreddit_name))\n",
    "    query_folder = os.path.join(subreddit_folder, sanitize_filename(search_query))\n",
    "    os.makedirs(query_folder, exist_ok=True)\n",
    "    \n",
    "    # Check for existing JSON files\n",
    "    try:\n",
    "        json_files = [f for f in os.listdir(query_folder) if f.endswith('.json')]\n",
    "        if len(json_files) > 1:\n",
    "            tqdm.write(f\"Skipping {subreddit_name}/{search_query} - contains {len(json_files)} JSON files\")\n",
    "            return 0\n",
    "    except FileNotFoundError:\n",
    "        pass  # Folder was just created and doesn't exist yet (unlikely but possible)\n",
    "\n",
    "    processed_count = 0\n",
    "    retry_count = 0\n",
    "    max_retries = 3\n",
    "    \n",
    "    with tqdm(total=100, desc=f\"{subreddit_name[:15]}/{search_query[:20]}\", leave=False) as pbar:\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                # Get submissions with progress bar\n",
    "                submissions = subreddit.search(search_query, limit=100)\n",
    "                \n",
    "                for submission in submissions:\n",
    "                    try:\n",
    "                        post_data = {\n",
    "                            'subreddit': subreddit_name,\n",
    "                            'query': search_query,\n",
    "                            'post_id': submission.id,\n",
    "                            'post_title': submission.title,\n",
    "                            'post_text': submission.selftext,\n",
    "                            'author': str(submission.author),\n",
    "                            'timestamp': submission.created_utc,\n",
    "                            'score': submission.score,\n",
    "                            'url': f\"https://www.reddit.com{submission.permalink}\",\n",
    "                            'comments': []\n",
    "                        }\n",
    "\n",
    "                        # Get comments\n",
    "                        try:\n",
    "                            submission.comments.replace_more(limit=0)\n",
    "                            comments = submission.comments.list()[:100]\n",
    "                            for comment in comments:\n",
    "                                post_data['comments'].append({\n",
    "                                    'comment_id': comment.id,\n",
    "                                    'author': str(comment.author),\n",
    "                                    'timestamp': comment.created_utc,\n",
    "                                    'score': comment.score,\n",
    "                                    'comment_text': comment.body\n",
    "                                })\n",
    "                        except Exception as e:\n",
    "                            tqdm.write(f\"Error processing comments for {submission.id}: {e}\")\n",
    "\n",
    "                        # Save data\n",
    "                        filename = f\"{submission.id}.json\"\n",
    "                        filepath = os.path.join(query_folder, filename)\n",
    "                        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                            json.dump(post_data, f, indent=4)\n",
    "\n",
    "                        processed_count += 1\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix_str(f\"Saved: {processed_count}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        tqdm.write(f\"Error processing submission {submission.id}: {e}\")\n",
    "                \n",
    "                return processed_count\n",
    "                \n",
    "            except praw.exceptions.APIException as e:\n",
    "                if e.error_type == \"RATELIMIT\":\n",
    "                    retry_count += 1\n",
    "                    wait_time = 60\n",
    "                    tqdm.write(f\"Rate limit hit. Waiting {wait_time}s (retry {retry_count}/{max_retries})\")\n",
    "                    time.sleep(wait_time)\n",
    "                    pbar.reset()  # Reset progress bar for retry\n",
    "                else:\n",
    "                    tqdm.write(f\"API error: {e}\")\n",
    "                    return 0\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Error: {e}\")\n",
    "                return 0\n",
    "    \n",
    "    tqdm.write(f\"Max retries reached for {subreddit_name}/{search_query}\")\n",
    "    return 0\n",
    "\n",
    "# Base directory to store the data\n",
    "output_folder_base = './reddit_data'\n",
    "os.makedirs(output_folder_base, exist_ok=True)\n",
    "\n",
    "# Process tasks with nested progress bars\n",
    "total_processed = 0\n",
    "with tqdm(tasks, desc=\"Total Progress\", position=0) as main_pbar:\n",
    "    for subreddit, query in main_pbar:\n",
    "        main_pbar.set_postfix_str(f\"Current: {subreddit[:15]}/{query[:20]}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        count = process_subreddit_query(subreddit, query)\n",
    "        total_processed += count\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        main_pbar.write(f\"Completed {subreddit}/{query}: {count} posts in {elapsed:.1f}s\")\n",
    "\n",
    "print(f\"\\nFinished. Total posts processed: {total_processed} across {len(tasks)} tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# till here we got the data and we stored it in the json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask\n",
    "from dask.delayed import delayed\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def process_single_file(file_path):\n",
    "    \"\"\"Process a single JSON file with strict type enforcement\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except:\n",
    "        # Return empty DataFrame with correct schema if file is corrupted\n",
    "        return pd.DataFrame(columns=[\n",
    "            'subreddit', 'post_title', 'post_text', \n",
    "            'author', 'timestamp', 'score', 'comment_text'\n",
    "        ]).astype({\n",
    "            'subreddit': 'object',\n",
    "            'post_title': 'object',\n",
    "            'post_text': 'object',\n",
    "            'author': 'object',\n",
    "            'timestamp': 'float64',\n",
    "            'score': 'int64',\n",
    "            'comment_text': 'object'\n",
    "        })\n",
    "\n",
    "    records = []\n",
    "    base_schema = {\n",
    "        'subreddit': '',\n",
    "        'post_title': '',\n",
    "        'post_text': '',\n",
    "        'author': '',\n",
    "        'timestamp': 0.0,\n",
    "        'score': 0,\n",
    "        'comment_text': None\n",
    "    }\n",
    "\n",
    "    # Process post data\n",
    "    try:\n",
    "        post_record = base_schema.copy()\n",
    "        post_record.update({\n",
    "            'subreddit': str(data.get('subreddit', '')),\n",
    "            'post_title': str(data.get('post_title', '')),\n",
    "            'post_text': str(data.get('post_text', '')),\n",
    "            'author': str(data.get('author', '')),\n",
    "            'timestamp': float(data.get('timestamp', 0)),\n",
    "            'score': int(float(data.get('score', 0)))  # Handle numeric strings\n",
    "        })\n",
    "        records.append(post_record)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Process comments\n",
    "    for comment in data.get('comments', []):\n",
    "        try:\n",
    "            comment_record = base_schema.copy()\n",
    "            comment_record.update({\n",
    "                'subreddit': str(data.get('subreddit', '')),\n",
    "                'post_title': str(data.get('post_title', '')),\n",
    "                'author': str(comment.get('author', '')),\n",
    "                'timestamp': float(comment.get('timestamp', 0)),\n",
    "                'score': int(float(comment.get('score', 0))),\n",
    "                'comment_text': str(comment.get('comment_text', ''))\n",
    "            })\n",
    "            records.append(comment_record)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Create DataFrame with enforced types\n",
    "    try:\n",
    "        df = pd.DataFrame(records)\n",
    "        return df.astype({\n",
    "            'subreddit': 'object',\n",
    "            'post_title': 'object',\n",
    "            'post_text': 'object',\n",
    "            'author': 'object',\n",
    "            'timestamp': 'float64',\n",
    "            'score': 'int64',\n",
    "            'comment_text': 'object'\n",
    "        })\n",
    "    except:\n",
    "        return pd.DataFrame(columns=base_schema.keys()).astype({\n",
    "            'subreddit': 'object',\n",
    "            'post_title': 'object',\n",
    "            'post_text': 'object',\n",
    "            'author': 'object',\n",
    "            'timestamp': 'float64',\n",
    "            'score': 'int64',\n",
    "            'comment_text': 'object'\n",
    "        })\n",
    "\n",
    "def process_all_files(base_path):\n",
    "    \"\"\"Process files with strict type enforcement and parallel processing\"\"\"\n",
    "    # Collect JSON files\n",
    "    json_files = []\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for f in files:\n",
    "            if f.endswith('.json'):\n",
    "                json_files.append(os.path.join(root, f))\n",
    "\n",
    "    # Define explicit schema for Dask\n",
    "    meta = pd.DataFrame(columns=[\n",
    "        'subreddit', 'post_title', 'post_text', \n",
    "        'author', 'timestamp', 'score', 'comment_text'\n",
    "    ]).astype({\n",
    "        'subreddit': 'object',\n",
    "        'post_title': 'object',\n",
    "        'post_text': 'object',\n",
    "        'author': 'object',\n",
    "        'timestamp': 'float64',\n",
    "        'score': 'int64',\n",
    "        'comment_text': 'object'\n",
    "    })\n",
    "\n",
    "    # Create delayed objects\n",
    "    delayed_dfs = [delayed(process_single_file)(fn) for fn in json_files]\n",
    "    \n",
    "    # Create Dask DataFrame with explicit meta\n",
    "    ddf = dd.from_delayed(delayed_dfs, meta=meta)\n",
    "    return ddf\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Dask\n",
    "    dask.config.set(scheduler='threads', num_workers=os.cpu_count())\n",
    "    \n",
    "    # Process data\n",
    "    ddf = process_all_files('./reddit_data')\n",
    "    \n",
    "    # Optimize partitioning\n",
    "    # Calculate reasonable number of partitions (1 partition per 100MB estimated)\n",
    "    n_partitions = max(1, len(ddf) // 100_000)  # Adjust based on your data size\n",
    "    ddf = ddf.repartition(npartitions=n_partitions)\n",
    "    \n",
    "    # Remove empty partitions\n",
    "    ddf = ddf.map_partitions(lambda df: df if not df.empty else None)\n",
    "    \n",
    "    with tqdm(desc=\"Processing & Writing\", total=len(ddf.divisions)) as pbar:\n",
    "        write_task = ddf.to_csv(\n",
    "            'reddit_nfl_2023-*.csv',\n",
    "            index=False,\n",
    "            mode='wt',\n",
    "            header=True,\n",
    "            sep=',',\n",
    "            compute=False\n",
    "        )\n",
    "        \n",
    "    # Write to CSV with progress tracking\n",
    "    with tqdm(desc=\"Writing CSV Partitions\", total=len(write_task)) as pbar:\n",
    "        # Convert list of delayed objects to Dask collection\n",
    "        write_bag = dask.bag.from_delayed(write_task)\n",
    "        \n",
    "        # Start computation in background\n",
    "        future = write_bag.persist()\n",
    "        \n",
    "    \n",
    "    print(\"\\nProcessing completed successfully!\")\n",
    "    print(f\"Total partitions created: {n_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging: 100%|██████████| 3870766/3870766 [00:32<00:00, 119409.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged successfully into single CSV file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def merge_csv_files(pattern, output_file):\n",
    "    \"\"\"Merge CSV files into one with proper comma handling\"\"\"\n",
    "    # Read all partitions with Dask\n",
    "    ddf = dd.read_csv(pattern, dtype='object', quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    # Convert to pandas DataFrame (if under 1GB RAM)\n",
    "    df = ddf.compute()\n",
    "    \n",
    "    # Write single file with proper CSV formatting\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        writer.writerow(df.columns)  # Write header\n",
    "        \n",
    "        # Write rows with progress\n",
    "        for row in tqdm(df.itertuples(index=False), total=len(df), desc=\"Merging\"):\n",
    "            writer.writerow(row)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # First delete existing files if needed\n",
    "    # delete_existing_csv_files()  # Use the previous deletion function\n",
    "    \n",
    "    # Merge files\n",
    "    merge_csv_files(\n",
    "        pattern='reddit_nfl_2023-*.csv',\n",
    "        output_file='reddit_nfl_2023_combined.csv'\n",
    "    )\n",
    "    \n",
    "    print(\"Merged successfully into single CSV file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 partition files to delete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting partitions: 100%|██████████| 38/38 [00:00<00:00, 280.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleted 38 partition files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def delete_partition_files(pattern):\n",
    "    \"\"\"Delete partitioned CSV files while keeping the merged file\"\"\"\n",
    "    # Get all files matching the pattern\n",
    "    partition_files = glob.glob(pattern)\n",
    "    \n",
    "    # Remove the merged file from deletion list if present\n",
    "    merged_file = 'reddit_nfl_2023_combined.csv'\n",
    "    files_to_delete = [f for f in partition_files if f != merged_file]\n",
    "    \n",
    "    if not files_to_delete:\n",
    "        print(\"No partition files found to delete\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(files_to_delete)} partition files to delete\")\n",
    "    \n",
    "    # Delete with confirmation\n",
    "    response = input(\"Delete all partition files? (y/n): \")\n",
    "    if response.lower() != 'y':\n",
    "        print(\"Deletion cancelled\")\n",
    "        return\n",
    "    \n",
    "    # Delete files with progress bar\n",
    "    for file_path in tqdm(files_to_delete, desc=\"Deleting partitions\"):\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError deleting {file_path}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nDeleted {len(files_to_delete)} partition files\")\n",
    "    \n",
    "delete_partition_files('reddit_nfl_2023-*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QlU4HqS6dU3f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>author</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>score</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[RCon14] QB EPA/play since Week 6: 1. Ryan Tan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170-162-61-16</td>\n",
       "      <td>1.637082e+09</td>\n",
       "      <td>122.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[RCon14] QB EPA/play since Week 6: 1. Ryan Tan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>imNotBoogerMcFarland</td>\n",
       "      <td>1.637082e+09</td>\n",
       "      <td>119.0</td>\n",
       "      <td>I thought Derrick Henry was the whole team tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[RCon14] QB EPA/play since Week 6: 1. Ryan Tan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>threwda1s</td>\n",
       "      <td>1.637082e+09</td>\n",
       "      <td>39.0</td>\n",
       "      <td>LIONS FAN WERE RIGHT. FIRE\\nLOMBARDI INTO THE SUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[RCon14] QB EPA/play since Week 6: 1. Ryan Tan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170-162-61-16</td>\n",
       "      <td>1.637082e+09</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Also of note: Rodgers air yards/att is lower t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[RCon14] QB EPA/play since Week 6: 1. Ryan Tan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maad-Dog</td>\n",
       "      <td>1.637086e+09</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Nothing to see here, Jimmy is a top 5 QB as we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870761</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Yates] The Chiefs offense in the second half:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Five2one521</td>\n",
       "      <td>1.676334e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>And a few missed holding calls. The eagles def...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870762</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Yates] The Chiefs offense in the second half:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rufusjonz</td>\n",
       "      <td>1.676376e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Easy when you never get called for offensive h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870763</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Yates] The Chiefs offense in the second half:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>istrx13</td>\n",
       "      <td>1.676310e+09</td>\n",
       "      <td>384.0</td>\n",
       "      <td>I wish Andy Reid would put me in a blender and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870764</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Yates] The Chiefs offense in the second half:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root88</td>\n",
       "      <td>1.676313e+09</td>\n",
       "      <td>152.0</td>\n",
       "      <td>A lot of people are trying to say the field wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870765</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Yates] The Chiefs offense in the second half:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mklugs</td>\n",
       "      <td>1.676321e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes that is what decided the game 100%\\n\\nEdit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3870766 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit                                         post_title  \\\n",
       "0             nfl  [RCon14] QB EPA/play since Week 6: 1. Ryan Tan...   \n",
       "1             nfl  [RCon14] QB EPA/play since Week 6: 1. Ryan Tan...   \n",
       "2             nfl  [RCon14] QB EPA/play since Week 6: 1. Ryan Tan...   \n",
       "3             nfl  [RCon14] QB EPA/play since Week 6: 1. Ryan Tan...   \n",
       "4             nfl  [RCon14] QB EPA/play since Week 6: 1. Ryan Tan...   \n",
       "...           ...                                                ...   \n",
       "3870761       nfl  [Yates] The Chiefs offense in the second half:...   \n",
       "3870762       nfl  [Yates] The Chiefs offense in the second half:...   \n",
       "3870763       nfl  [Yates] The Chiefs offense in the second half:...   \n",
       "3870764       nfl  [Yates] The Chiefs offense in the second half:...   \n",
       "3870765       nfl  [Yates] The Chiefs offense in the second half:...   \n",
       "\n",
       "        post_text                author     timestamp  score  \\\n",
       "0             NaN         170-162-61-16  1.637082e+09  122.0   \n",
       "1             NaN  imNotBoogerMcFarland  1.637082e+09  119.0   \n",
       "2             NaN             threwda1s  1.637082e+09   39.0   \n",
       "3             NaN         170-162-61-16  1.637082e+09   36.0   \n",
       "4             NaN              Maad-Dog  1.637086e+09   18.0   \n",
       "...           ...                   ...           ...    ...   \n",
       "3870761       NaN           Five2one521  1.676334e+09    0.0   \n",
       "3870762       NaN             rufusjonz  1.676376e+09    0.0   \n",
       "3870763       NaN               istrx13  1.676310e+09  384.0   \n",
       "3870764       NaN                root88  1.676313e+09  152.0   \n",
       "3870765       NaN                mklugs  1.676321e+09    2.0   \n",
       "\n",
       "                                              comment_text  \n",
       "0                                                      NaN  \n",
       "1        I thought Derrick Henry was the whole team tho...  \n",
       "2        LIONS FAN WERE RIGHT. FIRE\\nLOMBARDI INTO THE SUN  \n",
       "3        Also of note: Rodgers air yards/att is lower t...  \n",
       "4        Nothing to see here, Jimmy is a top 5 QB as we...  \n",
       "...                                                    ...  \n",
       "3870761  And a few missed holding calls. The eagles def...  \n",
       "3870762  Easy when you never get called for offensive h...  \n",
       "3870763  I wish Andy Reid would put me in a blender and...  \n",
       "3870764  A lot of people are trying to say the field wa...  \n",
       "3870765  Yes that is what decided the game 100%\\n\\nEdit...  \n",
       "\n",
       "[3870766 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Try to read the CSV file, handling potential errors\n",
    "try:\n",
    "    reddit_nfl_2023_data = pd.read_csv(\"/home/research/Big Data Bowl/text-processing/reddit_nfl_2023_combined.csv\", on_bad_lines='skip')  # Skip bad lines\n",
    "    df = pd.DataFrame(reddit_nfl_2023_data)\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error reading CSV: {e}\")\n",
    "    print(\"Attempting to fix the CSV file and retry...\")\n",
    "\n",
    "    # Attempt to fix potential issues with quotes and line breaks in the CSV\n",
    "    with open(\"/home/research/Big Data Bowl/text-processing/reddit_nfl_2023_combined.csv\", 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Replace unescaped quotes with escaped quotes and remove line breaks within quotes\n",
    "    fixed_lines = []\n",
    "    for line in lines:\n",
    "        line = line.replace('\"', '\"\"')  # Escape quotes\n",
    "        fixed_lines.append(line.replace('\\n', '\\\\n'))  # Replace line breaks\n",
    "\n",
    "    with open(\"/home/research/Big Data Bowl/text-processing/reddit_nfl_2023_combined.csv\", 'w', encoding='utf-8') as file:\n",
    "        file.writelines(fixed_lines)\n",
    "\n",
    "    # Try to read the CSV again after the fix\n",
    "    try:\n",
    "        reddit_nfl_2023_data = pd.read_csv(\"/home/research/Big Data Bowl/text-processing/reddit_nfl_2023_combined.csv\")\n",
    "        df = pd.DataFrame(reddit_nfl_2023_data)\n",
    "        print(\"CSV file fixed and successfully read.\")\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error reading CSV after fix: {e}\")\n",
    "        print(\"Please manually inspect the CSV file for issues.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4JyckhAfdYlk"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_reddit_text(text):\n",
    "    \"\"\"\n",
    "    Cleans Reddit text by removing unwanted patterns, markdown, and noise.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Basic cleaning\n",
    "    text = str(text).lower()  # Lowercase (optional, but often useful)\n",
    "    text = re.sub(r\"\\n|\\r\", \" \", text)  # Replace newlines with spaces\n",
    "    \n",
    "    # Remove Reddit-specific artifacts\n",
    "    text = re.sub(r\"\\[\\*.*?\\*\\]\", \"\", text)  # Remove [*citation needed*] patterns\n",
    "    text = re.sub(r\"\\[.*?\\]\\(.*?\\)\", \"\", text)  # Remove markdown links [text](url)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove raw URLs\n",
    "    text = re.sub(r\"/?r/\\w+\", \"\", text)  # Remove subreddit mentions (/r/nfl)\n",
    "    text = re.sub(r\"/?u/\\w+\", \"\", text)  # Remove user mentions (/u/username)\n",
    "    \n",
    "    # Remove special characters (keep basic punctuation)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?\\'\\\"\\-]\", \" \", text)\n",
    "    \n",
    "    # Handle HTML entities (if any)\n",
    "    text = BeautifulSoup(text, \"lxml\").get_text()\n",
    "    \n",
    "    # Collapse whitespace and trim\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# # Apply cleaning to relevant columns\n",
    "# df[\"cleaned_post_title\"] = df[\"post_title\"].apply(clean_reddit_text)\n",
    "# df[\"cleaned_post_text\"] = df[\"post_text\"].apply(clean_reddit_text) \n",
    "# df[\"cleaned_comment_text\"] = df[\"comment_text\"].apply(clean_reddit_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d5cc8699f542a1a7bab411a721f474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=30241), Label(value='0 / 30241')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca62091c7ed44c28f9d340b4fff0cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=30241), Label(value='0 / 30241')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a84e94a3f34605997285b19d3e627d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=30241), Label(value='0 / 30241')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84349/2634720681.py:27: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
      "\n",
      "    filehandle = open(your filename)\n",
      "\n",
      "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  text = BeautifulSoup(text, \"lxml\").get_text()\n",
      "/tmp/ipykernel_84349/2634720681.py:27: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
      "\n",
      "    filehandle = open(your filename)\n",
      "\n",
      "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  text = BeautifulSoup(text, \"lxml\").get_text()\n",
      "/tmp/ipykernel_84349/2634720681.py:27: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
      "\n",
      "    filehandle = open(your filename)\n",
      "\n",
      "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  text = BeautifulSoup(text, \"lxml\").get_text()\n",
      "/tmp/ipykernel_84349/2634720681.py:27: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
      "\n",
      "    filehandle = open(your filename)\n",
      "\n",
      "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  text = BeautifulSoup(text, \"lxml\").get_text()\n",
      "/tmp/ipykernel_84349/2634720681.py:27: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
      "\n",
      "    filehandle = open(your filename)\n",
      "\n",
      "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  text = BeautifulSoup(text, \"lxml\").get_text()\n",
      "/tmp/ipykernel_84349/2634720681.py:27: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
      "\n",
      "    filehandle = open(your filename)\n",
      "\n",
      "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  text = BeautifulSoup(text, \"lxml\").get_text()\n",
      "/tmp/ipykernel_84349/2634720681.py:27: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
      "\n",
      "    filehandle = open(your filename)\n",
      "\n",
      "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  text = BeautifulSoup(text, \"lxml\").get_text()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'pandarallel' has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 34\u001B[0m\n\u001B[1;32m     31\u001B[0m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcleaned_comment_text\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomment_text\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mparallel_apply(clean_reddit_text)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Optional: Shutdown workers (auto-handled on script exit)\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m \u001B[43mpandarallel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclose\u001B[49m()\n",
      "\u001B[0;31mAttributeError\u001B[0m: type object 'pandarallel' has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "# Install if needed: pip install swifter tqdm\n",
    "import swifter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dask.distributed import Client\n",
    "# Install first: pip install pandarallel\n",
    "from pandarallel import pandarallel\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize parallel processing with progress bar\n",
    "pandarallel.initialize(\n",
    "    nb_workers=128,  # Match your 512 cores\n",
    "    progress_bar=True,\n",
    "    verbose=1  # Show initialization status\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize parallel processing (explicitly for 512 cores)\n",
    "swifter.set_defaults(\n",
    "    npartitions=64,\n",
    "    progress_bar=True,\n",
    "    scheduler=\"processes\",\n",
    "    allow_dask_on_strings=True,  # Critical for text data\n",
    "    dask_threshold=1,  # Force Dask even for small DataFrames\n",
    ")\n",
    "\n",
    "# Clean all columns in parallel with progress\n",
    "df[\"cleaned_post_title\"] = df[\"post_title\"].parallel_apply(clean_reddit_text)\n",
    "df[\"cleaned_post_text\"] = df[\"post_text\"].parallel_apply(clean_reddit_text)\n",
    "df[\"cleaned_comment_text\"] = df[\"comment_text\"].parallel_apply(clean_reddit_text)\n",
    "\n",
    "# Optional: Shutdown workers (auto-handled on script exit)\n",
    "# pandarallel.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>author</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>score</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>cleaned_post_title</th>\n",
       "      <th>cleaned_post_text</th>\n",
       "      <th>cleaned_comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[RCon14] QB EPA/play since Week 6: 1. Ryan Tan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170-162-61-16</td>\n",
       "      <td>1.637082e+09</td>\n",
       "      <td>122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rcon14 qb epa play since week 6 1. ryan tanneh...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[RCon14] QB EPA/play since Week 6: 1. Ryan Tan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>imNotBoogerMcFarland</td>\n",
       "      <td>1.637082e+09</td>\n",
       "      <td>119.0</td>\n",
       "      <td>I thought Derrick Henry was the whole team tho...</td>\n",
       "      <td>rcon14 qb epa play since week 6 1. ryan tanneh...</td>\n",
       "      <td></td>\n",
       "      <td>i thought derrick henry was the whole team tho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[RCon14] QB EPA/play since Week 6: 1. Ryan Tan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>threwda1s</td>\n",
       "      <td>1.637082e+09</td>\n",
       "      <td>39.0</td>\n",
       "      <td>LIONS FAN WERE RIGHT. FIRE\\nLOMBARDI INTO THE SUN</td>\n",
       "      <td>rcon14 qb epa play since week 6 1. ryan tanneh...</td>\n",
       "      <td></td>\n",
       "      <td>lions fan were right. fire lombardi into the sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[RCon14] QB EPA/play since Week 6: 1. Ryan Tan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170-162-61-16</td>\n",
       "      <td>1.637082e+09</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Also of note: Rodgers air yards/att is lower t...</td>\n",
       "      <td>rcon14 qb epa play since week 6 1. ryan tanneh...</td>\n",
       "      <td></td>\n",
       "      <td>also of note rodgers air yards att is lower th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[RCon14] QB EPA/play since Week 6: 1. Ryan Tan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maad-Dog</td>\n",
       "      <td>1.637086e+09</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Nothing to see here, Jimmy is a top 5 QB as we...</td>\n",
       "      <td>rcon14 qb epa play since week 6 1. ryan tanneh...</td>\n",
       "      <td></td>\n",
       "      <td>nothing to see here, jimmy is a top 5 qb as we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                         post_title post_text  \\\n",
       "0       nfl  [RCon14] QB EPA/play since Week 6: 1. Ryan Tan...       NaN   \n",
       "1       nfl  [RCon14] QB EPA/play since Week 6: 1. Ryan Tan...       NaN   \n",
       "2       nfl  [RCon14] QB EPA/play since Week 6: 1. Ryan Tan...       NaN   \n",
       "3       nfl  [RCon14] QB EPA/play since Week 6: 1. Ryan Tan...       NaN   \n",
       "4       nfl  [RCon14] QB EPA/play since Week 6: 1. Ryan Tan...       NaN   \n",
       "\n",
       "                 author     timestamp  score  \\\n",
       "0         170-162-61-16  1.637082e+09  122.0   \n",
       "1  imNotBoogerMcFarland  1.637082e+09  119.0   \n",
       "2             threwda1s  1.637082e+09   39.0   \n",
       "3         170-162-61-16  1.637082e+09   36.0   \n",
       "4              Maad-Dog  1.637086e+09   18.0   \n",
       "\n",
       "                                        comment_text  \\\n",
       "0                                                NaN   \n",
       "1  I thought Derrick Henry was the whole team tho...   \n",
       "2  LIONS FAN WERE RIGHT. FIRE\\nLOMBARDI INTO THE SUN   \n",
       "3  Also of note: Rodgers air yards/att is lower t...   \n",
       "4  Nothing to see here, Jimmy is a top 5 QB as we...   \n",
       "\n",
       "                                  cleaned_post_title cleaned_post_text  \\\n",
       "0  rcon14 qb epa play since week 6 1. ryan tanneh...                     \n",
       "1  rcon14 qb epa play since week 6 1. ryan tanneh...                     \n",
       "2  rcon14 qb epa play since week 6 1. ryan tanneh...                     \n",
       "3  rcon14 qb epa play since week 6 1. ryan tanneh...                     \n",
       "4  rcon14 qb epa play since week 6 1. ryan tanneh...                     \n",
       "\n",
       "                                cleaned_comment_text  \n",
       "0                                                     \n",
       "1     i thought derrick henry was the whole team tho  \n",
       "2   lions fan were right. fire lombardi into the sun  \n",
       "3  also of note rodgers air yards att is lower th...  \n",
       "4  nothing to see here, jimmy is a top 5 qb as we...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "omq4dpQCdcYZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count per column:\n",
      "subreddit                     0\n",
      "post_title                  152\n",
      "post_text               3839864\n",
      "author                   493727\n",
      "timestamp                   152\n",
      "score                       152\n",
      "comment_text              43231\n",
      "cleaned_post_title            0\n",
      "cleaned_post_text             0\n",
      "cleaned_comment_text          0\n",
      "dtype: int64\n",
      "\n",
      "NaN percentage per column:\n",
      "subreddit                0.000000\n",
      "post_title               0.003927\n",
      "post_text               99.201657\n",
      "author                  12.755279\n",
      "timestamp                0.003927\n",
      "score                    0.003927\n",
      "comment_text             1.116859\n",
      "cleaned_post_title       0.000000\n",
      "cleaned_post_text        0.000000\n",
      "cleaned_comment_text     0.000000\n",
      "dtype: float64\n",
      "\n",
      "Total NaN count in the DataFrame: 4377278\n",
      "Total NaN percentage in the DataFrame: 11.31%\n"
     ]
    }
   ],
   "source": [
    "# Count NaN values in each column\n",
    "nan_count_per_column = df.isnull().sum()\n",
    "\n",
    "# Count total NaN values in the DataFrame\n",
    "total_nan_count = df.isnull().sum().sum()\n",
    "\n",
    "# Calculate the percentage of NaN values in each column\n",
    "nan_percentage_per_column = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Calculate the overall percentage of NaN values in the DataFrame\n",
    "total_nan_percentage = (total_nan_count / (df.shape[0] * df.shape[1])) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"NaN count per column:\")\n",
    "print(nan_count_per_column)\n",
    "\n",
    "print(\"\\nNaN percentage per column:\")\n",
    "print(nan_percentage_per_column)\n",
    "\n",
    "print(f\"\\nTotal NaN count in the DataFrame: {total_nan_count}\")\n",
    "print(f\"Total NaN percentage in the DataFrame: {total_nan_percentage:.2f}%\") #added percentage formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ryZkSVxndhrt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>author</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>score</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>cleaned_post_title</th>\n",
       "      <th>cleaned_post_text</th>\n",
       "      <th>cleaned_comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2107596</th>\n",
       "      <td>nfl</td>\n",
       "      <td>\"Contain Cam, Make Him Throw, Double Olsen\" - ...</td>\n",
       "      <td>For years, there's been a running joke amongst...</td>\n",
       "      <td>allsecretsknown</td>\n",
       "      <td>1.453793e+09</td>\n",
       "      <td>598.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"contain cam, make him throw, double olsen\" - ...</td>\n",
       "      <td>for years, there's been a running joke amongst...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085001</th>\n",
       "      <td>nfl</td>\n",
       "      <td>\"I am Mic'd up... I forgot about... I let you ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The_Blue_Rooster</td>\n",
       "      <td>1.636415e+09</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"i am mic'd up... i forgot about... i let you ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127924</th>\n",
       "      <td>nfl</td>\n",
       "      <td>\"I don't think he's a consistent thrower of th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pricklypearbear15</td>\n",
       "      <td>1.651164e+09</td>\n",
       "      <td>554.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"i don't think he's a consistent thrower of th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605530</th>\n",
       "      <td>nfl</td>\n",
       "      <td>\"Josh Allen is the GREATEST Football player on...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BungoPlease</td>\n",
       "      <td>1.734461e+09</td>\n",
       "      <td>84.0</td>\n",
       "      <td>&gt;The lack of mistakes and high number of passi...</td>\n",
       "      <td>\"josh allen is the greatest football player on...</td>\n",
       "      <td></td>\n",
       "      <td>the lack of mistakes and high number of passin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990844</th>\n",
       "      <td>nfl</td>\n",
       "      <td>\"Lacks desired size of an every down back\" - C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uniquecannon</td>\n",
       "      <td>1.556637e+09</td>\n",
       "      <td>815.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"lacks desired size of an every down back\" - c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619221</th>\n",
       "      <td>nfl</td>\n",
       "      <td>🃏NFL Wild Card Weekend Predictions Thread (202...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>_Vaudeville_</td>\n",
       "      <td>1.736354e+09</td>\n",
       "      <td>45.0</td>\n",
       "      <td>I’m convinced it’ll either be 30-10 Ravens or ...</td>\n",
       "      <td>nfl wild card weekend predictions thread 2025 ...</td>\n",
       "      <td></td>\n",
       "      <td>i m convinced it ll either be 30-10 ravens or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416713</th>\n",
       "      <td>nfl</td>\n",
       "      <td>🎃👻NFL Week 9 Predictions Thread (2024 Season)🧟...</td>\n",
       "      <td>Happy Halloween, /r/NFL! Week 9 is here and we...</td>\n",
       "      <td>baconlovr</td>\n",
       "      <td>1.730301e+09</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nfl week 9 predictions thread 2024 season</td>\n",
       "      <td>happy halloween, ! week 9 is here and we're lo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209622</th>\n",
       "      <td>nfl</td>\n",
       "      <td>🎅🎁NFL Week 17 Predictions Thread (2024 Season)☃️🎄</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daeshonbro</td>\n",
       "      <td>1.735061e+09</td>\n",
       "      <td>37.0</td>\n",
       "      <td>The packers Vikings game feels almost too clos...</td>\n",
       "      <td>nfl week 17 predictions thread 2024 season</td>\n",
       "      <td></td>\n",
       "      <td>the packers vikings game feels almost too clos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809355</th>\n",
       "      <td>nfl</td>\n",
       "      <td>🎉NFL Week 18 Predictions Thread (2024 Season)🎇</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TallEnoughJones</td>\n",
       "      <td>1.735757e+09</td>\n",
       "      <td>40.0</td>\n",
       "      <td>My predictions:\\n\\nBengals beat the Steelers\\n...</td>\n",
       "      <td>nfl week 18 predictions thread 2024 season</td>\n",
       "      <td></td>\n",
       "      <td>my predictions bengals beat the steelers jets ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788505</th>\n",
       "      <td>nfl</td>\n",
       "      <td>🦃🍂NFL Week 13 Predictions Thread (2024 Season)🥧🏈</td>\n",
       "      <td>NaN</td>\n",
       "      <td>athrowawayiguesslol</td>\n",
       "      <td>1.732728e+09</td>\n",
       "      <td>48.0</td>\n",
       "      <td>It seems weird to me when people bring up that...</td>\n",
       "      <td>nfl week 13 predictions thread 2024 season</td>\n",
       "      <td></td>\n",
       "      <td>it seems weird to me when people bring up that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12497 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit                                         post_title  \\\n",
       "2107596       nfl  \"Contain Cam, Make Him Throw, Double Olsen\" - ...   \n",
       "2085001       nfl  \"I am Mic'd up... I forgot about... I let you ...   \n",
       "3127924       nfl  \"I don't think he's a consistent thrower of th...   \n",
       "605530        nfl  \"Josh Allen is the GREATEST Football player on...   \n",
       "990844        nfl  \"Lacks desired size of an every down back\" - C...   \n",
       "...           ...                                                ...   \n",
       "619221        nfl  🃏NFL Wild Card Weekend Predictions Thread (202...   \n",
       "416713        nfl  🎃👻NFL Week 9 Predictions Thread (2024 Season)🧟...   \n",
       "2209622       nfl  🎅🎁NFL Week 17 Predictions Thread (2024 Season)☃️🎄   \n",
       "1809355       nfl     🎉NFL Week 18 Predictions Thread (2024 Season)🎇   \n",
       "1788505       nfl   🦃🍂NFL Week 13 Predictions Thread (2024 Season)🥧🏈   \n",
       "\n",
       "                                                 post_text  \\\n",
       "2107596  For years, there's been a running joke amongst...   \n",
       "2085001                                                NaN   \n",
       "3127924                                                NaN   \n",
       "605530                                                 NaN   \n",
       "990844                                                 NaN   \n",
       "...                                                    ...   \n",
       "619221                                                 NaN   \n",
       "416713   Happy Halloween, /r/NFL! Week 9 is here and we...   \n",
       "2209622                                                NaN   \n",
       "1809355                                                NaN   \n",
       "1788505                                                NaN   \n",
       "\n",
       "                      author     timestamp  score  \\\n",
       "2107596      allsecretsknown  1.453793e+09  598.0   \n",
       "2085001     The_Blue_Rooster  1.636415e+09  131.0   \n",
       "3127924    pricklypearbear15  1.651164e+09  554.0   \n",
       "605530           BungoPlease  1.734461e+09   84.0   \n",
       "990844          uniquecannon  1.556637e+09  815.0   \n",
       "...                      ...           ...    ...   \n",
       "619221          _Vaudeville_  1.736354e+09   45.0   \n",
       "416713             baconlovr  1.730301e+09   43.0   \n",
       "2209622           daeshonbro  1.735061e+09   37.0   \n",
       "1809355      TallEnoughJones  1.735757e+09   40.0   \n",
       "1788505  athrowawayiguesslol  1.732728e+09   48.0   \n",
       "\n",
       "                                              comment_text  \\\n",
       "2107596                                                NaN   \n",
       "2085001                                                NaN   \n",
       "3127924                                                NaN   \n",
       "605530   >The lack of mistakes and high number of passi...   \n",
       "990844                                                 NaN   \n",
       "...                                                    ...   \n",
       "619221   I’m convinced it’ll either be 30-10 Ravens or ...   \n",
       "416713                                                 NaN   \n",
       "2209622  The packers Vikings game feels almost too clos...   \n",
       "1809355  My predictions:\\n\\nBengals beat the Steelers\\n...   \n",
       "1788505  It seems weird to me when people bring up that...   \n",
       "\n",
       "                                        cleaned_post_title  \\\n",
       "2107596  \"contain cam, make him throw, double olsen\" - ...   \n",
       "2085001  \"i am mic'd up... i forgot about... i let you ...   \n",
       "3127924  \"i don't think he's a consistent thrower of th...   \n",
       "605530   \"josh allen is the greatest football player on...   \n",
       "990844   \"lacks desired size of an every down back\" - c...   \n",
       "...                                                    ...   \n",
       "619221   nfl wild card weekend predictions thread 2025 ...   \n",
       "416713           nfl week 9 predictions thread 2024 season   \n",
       "2209622         nfl week 17 predictions thread 2024 season   \n",
       "1809355         nfl week 18 predictions thread 2024 season   \n",
       "1788505         nfl week 13 predictions thread 2024 season   \n",
       "\n",
       "                                         cleaned_post_text  \\\n",
       "2107596  for years, there's been a running joke amongst...   \n",
       "2085001                                                      \n",
       "3127924                                                      \n",
       "605530                                                       \n",
       "990844                                                       \n",
       "...                                                    ...   \n",
       "619221                                                       \n",
       "416713   happy halloween, ! week 9 is here and we're lo...   \n",
       "2209622                                                      \n",
       "1809355                                                      \n",
       "1788505                                                      \n",
       "\n",
       "                                      cleaned_comment_text  \n",
       "2107596                                                     \n",
       "2085001                                                     \n",
       "3127924                                                     \n",
       "605530   the lack of mistakes and high number of passin...  \n",
       "990844                                                      \n",
       "...                                                    ...  \n",
       "619221   i m convinced it ll either be 30-10 ravens or ...  \n",
       "416713                                                      \n",
       "2209622  the packers vikings game feels almost too clos...  \n",
       "1809355  my predictions bengals beat the steelers jets ...  \n",
       "1788505  it seems weird to me when people bring up that...  \n",
       "\n",
       "[12497 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_score_df = df.loc[df.groupby('post_title')['score'].idxmax()]\n",
    "\n",
    "highest_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='reddit_content')]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "# Replace with your details\n",
    "api_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIiwiZXhwIjoxNzUyMDA5MTQwfQ.BpKQ-ZrWfPPca_GB5v1enA8nZKI_bez2ahmD73sAcu4\"\n",
    "url = \"https://3d184921-7bd2-4bff-9699-8bbc5c2999f2.us-east4-0.gcp.cloud.qdrant.io\"\n",
    "\n",
    "client = QdrantClient(url=url, api_key=api_key)\n",
    "\n",
    "# Verify connection\n",
    "print(client.get_collections())  # Should return existing collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84349/1860951971.py:1: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.recreate_collection(\n",
    "    collection_name=\"reddit_content\",\n",
    "    vectors_config={\n",
    "        \"title_vector\": models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
    "        \"post_vector\": models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
    "        \"comment_vector\": models.VectorParams(size=384, distance=models.Distance.COSINE)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec12928c1934402bac0b0a6db9a33308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GPU Encoding:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b79c4dccb884c808380bc038feba22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fb7d19991c47b4b41f706c9f2e2d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b3dfd017a648029860321acccaf081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 45\u001B[0m\n\u001B[1;32m     42\u001B[0m     pbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Verify\u001B[39;00m\n\u001B[0;32m---> 45\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtitle_emb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m)  \u001B[38;5;66;03m# Should show (384,)\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal entries: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(df)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Should match your cleaned data size\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "BATCH_SIZE = 1024 * NUM_GPUS  # Adjust based on your GPU memory\n",
    "\n",
    "# Clean and validate data FIRST\n",
    "df = df.dropna(subset=[\"cleaned_post_title\", \"cleaned_comment_text\"])\n",
    "df = df.fillna({\"cleaned_post_text\": \"\"})\n",
    "df = df.reset_index(drop=True)  # Critical for alignment\n",
    "\n",
    "def parallel_encode(texts):\n",
    "    \"\"\"Process embeddings with GPU optimization\"\"\"\n",
    "    model = SentenceTransformer(MODEL_NAME).to(\"cuda\")\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    return [emb.tolist() for emb in embeddings]  # Convert to list of lists\n",
    "\n",
    "# Encode with proper DataFrame assignment\n",
    "with tqdm(total=3, desc=\"GPU Encoding\") as pbar:\n",
    "    # Titles\n",
    "    df[\"title_emb\"] = parallel_encode(df[\"cleaned_post_title\"].tolist())\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Posts \n",
    "    df[\"post_emb\"] = parallel_encode(df[\"cleaned_post_text\"].tolist())\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Comments\n",
    "    df[\"comment_emb\"] = parallel_encode(df[\"cleaned_comment_text\"].tolist())\n",
    "    pbar.update(1)\n",
    "\n",
    "# Verify\n",
    "print(df[\"title_emb\"][0].shape)  # Should show (384,)\n",
    "print(f\"Total entries: {len(df)}\")  # Should match your cleaned data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"intermediate.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({\"title_emb\":\"post_title_emb\", \"post_emb\":\"post_text_emb\", \"comment_emb\":\"comment_text_emb\"}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing COMMENT_TEXT ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84349/4118504218.py:3: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1039008 valid comment_text entries\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc4c7b496be402e85253e2b83fb3dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading comment_texts:   0%|          | 0/1039008 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed comment_text upload\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_collection(client, collection_name):\n",
    "    \"\"\"Create optimized collection with single vector\"\"\"\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
    "        optimizers_config=models.OptimizersConfigDiff(\n",
    "        deleted_threshold=0.5,\n",
    "        indexing_threshold=10000,\n",
    "        memmap_threshold=20000,\n",
    "        vacuum_min_vector_number=1000,\n",
    "        default_segment_number=2,\n",
    "        flush_interval_sec=10\n",
    "    )\n",
    "\n",
    "    )\n",
    "\n",
    "def prepare_batch(df_chunk, content_type):\n",
    "    \"\"\"Prepare points for specific content type with NaN filtering\"\"\"\n",
    "    points = []\n",
    "    for idx, row in df_chunk.iterrows():\n",
    "        # Skip empty content\n",
    "        if pd.isna(row[f\"cleaned_{content_type}\"]) or row[f\"cleaned_{content_type}\"] == \"\":\n",
    "            continue\n",
    "            \n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=idx,\n",
    "                vector=np.array(row[f\"{content_type}_emb\"]).astype(np.float32).tolist(),\n",
    "                payload={\n",
    "                    \"text\": row[f\"cleaned_{content_type}\"]\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return points\n",
    "\n",
    "def upload_content_type(df, content_type, BATCH_SIZE=32):\n",
    "    \"\"\"Full upload pipeline for a content type\"\"\"\n",
    "    client = QdrantClient(url=url, api_key=api_key)\n",
    "    collection_name = f\"reddit_{content_type}s\"\n",
    "    \n",
    "    print(f\"\\n=== Processing {content_type.upper()} ===\")\n",
    "    create_collection(client, collection_name)\n",
    "    \n",
    "    \n",
    "    col_name = f\"cleaned_{content_type}\"\n",
    "    emb_col = f\"{content_type}_emb\"\n",
    "    \n",
    "    valid_df = df[\n",
    "        df[col_name].notna() & \n",
    "        (df[col_name] != \"\")\n",
    "    ].drop_duplicates(subset=[col_name]).copy()\n",
    "    \n",
    "    print(f\"Found {len(valid_df)} valid {content_type} entries\")\n",
    "    \n",
    "    total_points = len(valid_df)\n",
    "    progress_bar = tqdm(total=total_points, desc=f\"Uploading {content_type}s\")\n",
    "    \n",
    "    for start_idx in range(0, total_points, BATCH_SIZE):\n",
    "        end_idx = min(start_idx + BATCH_SIZE, total_points)\n",
    "        batch = valid_df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        points = prepare_batch(batch, content_type)\n",
    "        if not points:\n",
    "            continue\n",
    "            \n",
    "        client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points,\n",
    "            wait=True\n",
    "        )\n",
    "        progress_bar.update(len(points))\n",
    "        \n",
    "    progress_bar.close()\n",
    "    print(f\"Completed {content_type} upload\\n\")\n",
    "\n",
    "# Main execution flow\n",
    "if __name__ == \"__main__\":\n",
    "    # Convert embeddings first\n",
    "    for col in [\"post_title_emb\",\t\"post_text_emb\",\t\"comment_text_emb\"]:\n",
    "        df[col] = df[col].apply(lambda x: np.array(x).astype(np.float32))\n",
    "    \n",
    "    # Upload in priority order\n",
    "    upload_content_type(df, \"post_title\")\n",
    "    upload_content_type(df, \"post_text\") \n",
    "    upload_content_type(df, \"comment_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16321bf30f5149e6a1abefa0a6f8781a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d0ef8669cb4176b442809cc766ab9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84349/2165995955.py:39: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f15a4ce4cc4971a5fb59e0a9f62e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa493e01b1948ff9d4bfc0c2443dbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(MODEL_NAME).to(\"cuda\")\n",
    "\n",
    "def process_dataset(df: pd.DataFrame, config: dict) -> pd.DataFrame:\n",
    "    \"\"\"Process and embed a dataset based on configuration\"\"\"\n",
    "    # 1. Clean and deduplicate\n",
    "    # text_col = config['text_column']\n",
    "    \n",
    "    # df_clean = df[\n",
    "    #     df[text_col].notna() & \n",
    "    #     (df[text_col] != \"\")\n",
    "    # ].drop_duplicates(subset=[text_col]).copy()\n",
    "    text_col = config['text_column']\n",
    "    df_clean = df\n",
    "    \n",
    "    # 2. Generate embeddings and convert to list so that each row gets a 1D array\n",
    "    df_clean['combined_text'] = df_clean.apply(\n",
    "    lambda row: f\"{row['Name']}. {row.get('Strengths', '')} {row.get('Weaknesses', '')} {row[text_col]}\",\n",
    "    axis=1\n",
    "    )\n",
    "    embeddings = model.encode(df_clean['combined_text'].tolist(), show_progress_bar=True)\n",
    "    df_clean['embeddings'] = list(embeddings)\n",
    "\n",
    "    \n",
    "    # 3. Prepare metadata with error handling\n",
    "    df_clean['payload'] = df_clean.apply(\n",
    "        lambda row: {\n",
    "            key: row.get(col, None)  # Handle missing columns gracefully\n",
    "            for key, col in config['payload_map'].items()\n",
    "        },\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df_clean[['embeddings', 'payload']]\n",
    "\n",
    "\n",
    "def upload_dataset(client: QdrantClient, collection_name: str, df: pd.DataFrame):\n",
    "    \"\"\"Upload processed dataset to Qdrant\"\"\"\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
    "        optimizers_config=models.OptimizersConfigDiff(\n",
    "            indexing_threshold=10000,\n",
    "            memmap_threshold=20000\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    points = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        points.append(models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=row['embeddings'].tolist(),\n",
    "            payload=row['payload']\n",
    "        ))\n",
    "        \n",
    "        if len(points) >= BATCH_SIZE:\n",
    "            client.upsert(\n",
    "                collection_name=collection_name,\n",
    "                points=points,\n",
    "                wait=False\n",
    "            )\n",
    "            points = []\n",
    "    \n",
    "    if points:  # Final batch\n",
    "        client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points,\n",
    "            wait=True\n",
    "        )\n",
    "\n",
    "# Process Scouting Reports\n",
    "scouting_config = {\n",
    "    'text_column': 'Cleaned_report',\n",
    "    'payload_map': {\n",
    "        'name': 'Name',\n",
    "        'report': 'Cleaned_report'\n",
    "    }\n",
    "}\n",
    "scouting_df = process_dataset(\n",
    "    pd.read_csv('scouting_reports_with_text.csv'),\n",
    "    scouting_config\n",
    ")\n",
    "\n",
    "# Process NFL Strengths\n",
    "nfl_config = {\n",
    "    'text_column': 'Summary',\n",
    "    'payload_map': {\n",
    "        'name': 'Name',\n",
    "        'strengths': 'Strengths',\n",
    "        'weaknesses': 'Weaknesses',\n",
    "        'summary': 'Summary'\n",
    "    }\n",
    "}\n",
    "nfl_df = process_dataset(\n",
    "    pd.read_csv('nfl_players_with_strengths_weaknesses_summary_all_pages.csv'), \n",
    "    nfl_config\n",
    ")\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client = QdrantClient(url=url, api_key=api_key)\n",
    "\n",
    "# Upload to separate collections\n",
    "upload_dataset(client, \"scouting_reports\", scouting_df)\n",
    "upload_dataset(client, \"nfl_analyses\", nfl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYjm2SJGdpOR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 20:31:01.327636: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-28 20:31:01.410995: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743211861.466282   22021 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743211861.478810   22021 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-28 20:31:01.562663: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import faiss\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "# from datasets import load_dataset, Dataset # Import Dataset class\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import pandas as pd\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caaj8LG-do-f"
   },
   "outputs": [],
   "source": [
    "# #Combine title and comment text for embedding\n",
    "\n",
    "# df['combined_text'] = df['subreddit'] + df['post_title'] + df['comment_text'].fillna('')\n",
    "# texts = df['combined_text']\n",
    "\n",
    "# # Load embedding model for retrieval\n",
    "# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# # Convert text to embeddings\n",
    "# embeddings = embedding_model.encode(texts.tolist(), convert_to_numpy=True)\n",
    "\n",
    "# # Create FAISS index for retrieval\n",
    "# index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "# index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvSDGxoEdyf5"
   },
   "outputs": [],
   "source": [
    "# # Load an LLM for fine-tuning (GPT-2)\n",
    "# model_name = \"gpt2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# # Add padding token to tokenizer\n",
    "# tokenizer.pad_token = tokenizer.eos_token  # or '[PAD]' if you prefer\n",
    "\n",
    "# # Function to retrieve relevant documents based on a query\n",
    "# def retrieve_docs(query, top_k=3):\n",
    "#     query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "#     _, indices = index.search(query_embedding, top_k)\n",
    "#     return [texts.iloc[i] for i in indices[0]]\n",
    "\n",
    "# # Fine-tune the LLM with subreddit data\n",
    "# def fine_tune_llm(df, model, tokenizer):\n",
    "#     def tokenize_function(examples):\n",
    "#         return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "#     dataset = Dataset.from_pandas(df)\n",
    "#     dataset = dataset.rename_column(\"combined_text\", \"text\")\n",
    "#     tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "#     # Import DataCollatorForLanguageModeling #Adding missing import\n",
    "#     from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "#     data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=\"./fine_tuned_subreddit_model\",\n",
    "#         overwrite_output_dir=True,\n",
    "#         num_train_epochs=3,  # Adjust epochs as needed\n",
    "#         per_device_train_batch_size=8,  # Adjust batch size as needed\n",
    "#         save_steps=10_000,\n",
    "#         save_total_limit=2,\n",
    "#         prediction_loss_only=True,\n",
    "#         report_to=\"none\", # remove for wandb or tensorboard\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         data_collator=data_collator,\n",
    "#         train_dataset=tokenized_datasets,\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "#     trainer.save_model(\"./fine_tuned_subreddit_model\")\n",
    "\n",
    "# # Uncomment the line below to run fine-tuning (can take time)\n",
    "# fine_tune_llm(df, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iKEeyLEd1Yf"
   },
   "outputs": [],
   "source": [
    "# def generate_answer(query, model, tokenizer):\n",
    "#     retrieved_docs = retrieve_docs(query)\n",
    "#     input_text = \"\\n\".join(retrieved_docs) + \"\\nQuestion: \" + query + \"\\nAnswer: \"\n",
    "\n",
    "#     inputs = tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=True)\n",
    "#     input_ids = inputs.input_ids\n",
    "#     attention_mask = inputs.attention_mask\n",
    "\n",
    "#     # Move inputs to the same device as the model\n",
    "#     device = model.device  # Get the device of the model (CPU or GPU)\n",
    "#     input_ids = input_ids.to(device)\n",
    "#     attention_mask = attention_mask.to(device)\n",
    "\n",
    "#     output = model.generate(input_ids, attention_mask=attention_mask, max_length=500)\n",
    "#     answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#     return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tl8_aoUQd53i"
   },
   "outputs": [],
   "source": [
    "# query = \"What was the score of Super Bowl LVII?\"\n",
    "# answer = generate_answer(query, model, tokenizer)\n",
    "# print(\"Q:\", query)\n",
    "# print(\"A:\", answer)\n",
    "\n",
    "# # Load the fine tuned model if it was saved\n",
    "# fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_subreddit_model\")\n",
    "# answer_fine_tuned = generate_answer(query, fine_tuned_model, tokenizer)\n",
    "# print(\"Q:\", query)\n",
    "# print(\"A: (Fine-tuned)\", answer_fine_tuned)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
